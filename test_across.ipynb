{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0466b7-c78e-4710-8aa2-08aad85b2c54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU is available.\n",
      "Current GPU device: NVIDIA A100-PCIE-40GB\n",
      "Total RAM: 1006.92 GB\n",
      "Available RAM: 958.37 GB\n"
     ]
    }
   ],
   "source": [
    "# Setup Environment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import time\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA GPU is available.\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"CUDA GPU is not available. Using CPU instead.\")\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f\"Current GPU device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "print(f\"Total RAM: {(psutil.virtual_memory().total / (1024**3)):.2f} GB\")\n",
    "print(f\"Available RAM: {(psutil.virtual_memory().available / (1024**3)):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca50642-9a7d-4427-9d08-e7ec5e9bfc86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train model on mouse 1, save best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf6785b-6fb4-41e9-b178-37220d73dcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /nas/longleaf/home/cvita1/.netrc\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "user = wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d316ed22-9fa9-40d1-9ff0-2464ac64a90f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-15 12:24:52</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:17.38        </td></tr>\n",
       "<tr><td>Memory:      </td><td>50.2/1006.9 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None | Iter 5.000: None<br>Bracket: Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None<br>Bracket: Iter 80.000: None | Iter 40.000: None | Iter 20.000: None<br>Logical resource usage: 256.0/256 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  spatial_hidden_dim</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>_trainable_6c2ed48c</td><td>RUNNING </td><td>172.26.114.160:309396</td><td style=\"text-align: right;\">                  16</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_trainable pid=309396)\u001b[0m Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m X_val shape: torch.Size([476, 10, 2065, 1])\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m y_val shape: torch.Size([476, 1])\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m  \n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m X (array): Matrix of number of batch_size, time_steps_per_frame, num_nodes, and number of features per node.\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m X.shape = (B, T, N, F)\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m X_train shape: torch.Size([4284, 10, 2065, 1])\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m X_test shape: torch.Size([1190, 10, 2065, 1])\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m X_train type: <class 'torch.Tensor'>\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m  \n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m X_val shape: torch.Size([476, 10, 2065, 1])\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m y_val shape: torch.Size([476, 1])\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m  \n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m y_shape = [batch_size, unique_frames_shown_per_10_timesteps]\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m y_train shape: torch.Size([4284, 1])\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m y_test shape: torch.Size([1190, 1])\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m y_test type: <class 'torch.Tensor'>\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=309586)\u001b[0m wandb: Currently logged in as: rayscarpenter (neuropixel-unc). Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=309586)\u001b[0m wandb: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[36m(_WandbLoggingActor pid=309586)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[36m(_WandbLoggingActor pid=309586)\u001b[0m wandb: Tracking run with wandb version 0.16.2\n",
      "\u001b[36m(_WandbLoggingActor pid=309586)\u001b[0m wandb: Run data is saved locally in /nas/longleaf/home/cvita1/ray_results/exp/_trainable_6c2ed48c_1_Architecture=Static_STGAT,accumulation_steps=4,batch_size=16,early_stop_delta=0.0100,early_stop_patience=3,e_2024-04-15_12-22-35/wandb/run-20240415_122248-6c2ed48c\n",
      "\u001b[36m(_WandbLoggingActor pid=309586)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=309586)\u001b[0m wandb: Syncing run _trainable_6c2ed48c\n",
      "\u001b[36m(_WandbLoggingActor pid=309586)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/neuropixel-unc/Predicting%20Visual%20Stimulus\n",
      "\u001b[36m(_WandbLoggingActor pid=309586)\u001b[0m wandb: 🚀 View run at https://wandb.ai/neuropixel-unc/Predicting%20Visual%20Stimulus/runs/6c2ed48c\n",
      "Train Epoch 1/50:   0%|          | 0/268 [00:00<?, ?it/s]\n",
      "Train Epoch 1/50:   0%|          | 0/268 [00:50<?, ?it/s, Loss=1.21, Train Acc=0.724]\n",
      "Test Epoch 1/50:   0%|          | 0/75 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_trainable pid=309396)\u001b[0m Test accuracy 0.84% is not higher than the existing highest accuracy 85.21%. Model not saved.\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m Epoch 1, Duration: 55.87s, Loss: 1.21, Train Acc: 0.72%, Test Acc: 0.84%\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m Total connections: 4264225\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m Edge sliver weights: tensor([0.7311, 0.5004, 0.5064, 0.5009, 0.5022], device='cuda:0',\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m        grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 1/50:   0%|          | 0/75 [00:05<?, ?it/s]\n",
      "Train Epoch 2/50:   0%|          | 0/268 [00:00<?, ?it/s]\n",
      "Train Epoch 2/50:   0%|          | 0/268 [00:49<?, ?it/s, Loss=1.21, Train Acc=0.49]\n",
      "Test Epoch 2/50:   0%|          | 0/75 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_trainable pid=309396)\u001b[0m Epoch 2, Duration: 55.46s, Loss: 1.21, Train Acc: 0.49%, Test Acc: 0.67%\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m Total connections: 4264225\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m Edge sliver weights: tensor([0.7311, 0.5004, 0.5064, 0.5009, 0.5022], device='cuda:0',\n",
      "\u001b[36m(_trainable pid=309396)\u001b[0m        grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 2/50:   0%|          | 0/75 [00:05<?, ?it/s]\n",
      "Train Epoch 3/50:   0%|          | 0/268 [00:00<?, ?it/s]\n",
      "2024-04-15 12:24:52,942\tWARNING tune.py:186 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    }
   ],
   "source": [
    "from auto_hyperparameter_tuner import *\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "param_space = {\n",
    "    \"wandb_project\": \"Predicting Visual Stimulus\",\n",
    "    \"wandb_api_key\": \"7c8d251196fd96d2a93bfb6ffd0005ac030ce42b\",\n",
    "    \"num_epochs\": 50,\n",
    "    \"lr\": 0.000815819,\n",
    "    \"temporal_hidden_dim\": 760,\n",
    "    \"spatial_hidden_dim\": tune.randint(15,20),\n",
    "    \"edge_threshold\":.25,\n",
    "    \"early_stop_patience\": 3,\n",
    "    \"early_stop_delta\": 0.01,\n",
    "    \"batch_size\": 16,\n",
    "    \"graph_batch_size\": 16,\n",
    "    \"temporal_layer_dimension\":1,\n",
    "    \"spatial_out_features\": 1,\n",
    "    \"mouse_number\": 715093703,\n",
    "    \"timesteps\": 10,\n",
    "    \"Architecture\": 'Static_STGAT',\n",
    "    \"num_samples\": 1,\n",
    "    \"accumulation_steps\": 4,  \n",
    "    \"graph_lr\": .05, \n",
    "    \"use_auto_corr_matrix\": True,\n",
    "    \"file_path\":\"/proj/STOR/pipiras/Neuropixel/output/spike_trains_with_stimulus_session_715093703_10.pkl\"\n",
    "    }\n",
    "\n",
    "trainer = ModelTrainer(param_space)\n",
    "trainer.execute_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ccf39-e83c-445a-87f5-c07d002077bb",
   "metadata": {},
   "source": [
    "## Test loaded model on mouse 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b57e9-3e4a-46e9-a935-e847a1e8aa98",
   "metadata": {},
   "source": [
    "### Load data from mouse 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f21322-ab13-4e3c-8f73-3b0d9fe1e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_new_mouse_loader(file_path, batch_size=32):\n",
    "    \"\"\"\n",
    "    Loads data for a new mouse and returns a DataLoader for testing.\n",
    "\n",
    "    Args:\n",
    "        - file_path (str): Path to the new mouse data file.\n",
    "        - batch_size (int): Batch size for the DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        - DataLoader: A DataLoader instance containing the new mouse's data.\n",
    "        - num_nodes (int): Number of nodes in the data.\n",
    "        - num_classes (int): Number of unique classes in the data.\n",
    "        - spatial_in_features (int): Number of spatial input features.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if np.issubdtype(data['frame'].dtype, np.object_):\n",
    "        encoder = LabelEncoder()\n",
    "        y_encoded = encoder.fit_transform(data['frame'].values)\n",
    "        y = torch.tensor(y_encoded, dtype=torch.long)\n",
    "    else:\n",
    "        y = torch.tensor(data['frame'].values, dtype=torch.float32)\n",
    "\n",
    "    X = torch.tensor(data.drop(columns=['frame']).values, dtype=torch.float32)\n",
    "    num_samples = X.size(0)\n",
    "    num_nodes = X.size(1)\n",
    "    spatial_in_features = 1  \n",
    "    num_timesteps = 1 \n",
    "    print(num_nodes)\n",
    "\n",
    "\n",
    "    # Reshape X to have dimensions (num_samples, num_timesteps, num_nodes, spatial_in_features)\n",
    "    X = X.view(num_samples, num_timesteps, num_nodes, spatial_in_features)\n",
    "\n",
    "    num_classes = len(torch.unique(y))\n",
    "\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return loader, num_nodes, num_classes, spatial_in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a0e5425-72ad-4585-9c21-c138e496c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_hyperparameter_tuner import *\n",
    "\n",
    "param_space = {\n",
    "    \"wandb_project\": \"Predicting Visual Stimulus\",\n",
    "    \"wandb_api_key\": \"7c8d251196fd96d2a93bfb6ffd0005ac030ce42b\",\n",
    "    \"num_epochs\": 50,\n",
    "    \"lr\": 0.000815819,\n",
    "    \"temporal_hidden_dim\": 760,\n",
    "    \"spatial_hidden_dim\": int(tune.randint(15,20).sample()), \n",
    "    \"edge_threshold\":.25,\n",
    "    \"early_stop_patience\": 3,\n",
    "    \"early_stop_delta\": 0.01,\n",
    "    \"batch_size\": 16,\n",
    "    \"graph_batch_size\": 16,\n",
    "    \"temporal_layer_dimension\":1,\n",
    "    \"spatial_out_features\": int(1),  \n",
    "    \"mouse_number\": 715093703,\n",
    "    \"timesteps\": 10,\n",
    "    \"Architecture\": 'Static_STGAT',\n",
    "    \"num_samples\": 1,\n",
    "    \"accumulation_steps\": 4,\n",
    "    \"graph_lr\": .05,\n",
    "    \"use_auto_corr_matrix\": True,\n",
    "    \"file_path\":\"/proj/STOR/pipiras/Neuropixel/output/spike_trains_with_stimulus_session_715093703_1.pkl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d38bc7-1f85-4ce6-b4e7-78b45c417f54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2717\n",
      "Epoch 1, Loss: 3.0845, Train Acc: 36.76%\n",
      "Epoch 2, Loss: 1.9435, Train Acc: 59.94%\n",
      "Epoch 3, Loss: 1.3848, Train Acc: 70.40%\n",
      "Epoch 4, Loss: 0.9945, Train Acc: 78.22%\n",
      "Testing the loaded model on new mouse data...\n",
      "Number of nodes: 2717\n",
      "Number of classes: 119\n",
      "Input data shape: torch.Size([32, 1, 2717, 1])\n",
      "Accuracy of the model on the new mouse data: 77.27%\n"
     ]
    }
   ],
   "source": [
    "from auto_hyperparameter_tuner import *\n",
    "\n",
    "file_path = \"output/spike_trains_with_stimulus_session_732592105_10.pkl\"\n",
    "best_model_path = \"saved_models/Static_STGAT/best_model_Static_STGAT_715093703_85.21.pth\"\n",
    "\n",
    "trainer = ModelTrainer(param_space)\n",
    "new_mouse_loader, num_nodes, num_classes, spatial_in_features = get_new_mouse_loader(file_path)\n",
    "\n",
    "# Load saved model weights\n",
    "loaded_weights = torch.load(best_model_path)\n",
    "\n",
    "trainer.computed_params['num_classes'] = num_classes\n",
    "trainer.computed_params['spatial_in_features'] = spatial_in_features\n",
    "trainer.computed_params['num_nodes'] = num_nodes\n",
    "trainer.computed_params['lstm_input_dim'] = spatial_in_features * num_nodes\n",
    "trainer.computed_params['temporal_hidden_dim'] = param_space.get('temporal_hidden_dim', 128)\n",
    "trainer.computed_params['temporal_layer_dimension'] = param_space.get('temporal_layer_dimension', 1)\n",
    "trainer.computed_params['num_epochs'] = param_space.get('num_epochs', 10)\n",
    "\n",
    "# Fine-tune model on subset of new mouse data (20/80 split)\n",
    "subset_indices = np.random.choice(len(new_mouse_loader.dataset), size=int(0.8 * len(new_mouse_loader.dataset)), replace=False)\n",
    "subset_dataset = torch.utils.data.Subset(new_mouse_loader.dataset, subset_indices)\n",
    "subset_loader = torch.utils.data.DataLoader(subset_dataset, batch_size=param_space[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Get X_train from the subset data\n",
    "X_train = subset_dataset[:][0] \n",
    "\n",
    "# Initialize new model with same architecture as loaded model\n",
    "new_model = trainer.initialize_model(param_space, torch.device(\"cuda\"), num_nodes, X_train)\n",
    "\n",
    "# Load weights of matching layers from saved model\n",
    "state_dict = new_model.state_dict()\n",
    "loaded_state_dict = {k: v for k, v in loaded_weights.items() if k in state_dict and state_dict[k].size() == v.size()}\n",
    "state_dict.update(loaded_state_dict)\n",
    "new_model.load_state_dict(state_dict)\n",
    "\n",
    "def fine_tune_model(model, data_loader, config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    num_epochs = 4  # Num of fine-tuning epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for features, labels in data_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            labels = labels.squeeze().long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss / len(data_loader):.4f}, Train Acc: {train_acc:.2f}%')\n",
    "\n",
    "# Fine-tune model\n",
    "fine_tune_model(new_model, subset_loader, param_space)\n",
    "\n",
    "# Test fine-tuned model on new mouse data\n",
    "trainer.test_loaded_model(new_model, new_mouse_loader, num_nodes, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f09a88-1b10-43c0-8ac6-e08b6ccaeb4c",
   "metadata": {},
   "source": [
    "### Freeze feature extraction layers during fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e7fe8-3441-4c26-a691-8cb1e60e9a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2717\n",
      "Total number of layers in the model: 3\n",
      "Epoch 1, Loss: 3.1294, Train Acc: 36.84%\n",
      "Epoch 2, Loss: 1.9639, Train Acc: 60.54%\n",
      "Epoch 3, Loss: 1.3289, Train Acc: 72.19%\n",
      "Epoch 4, Loss: 0.8122, Train Acc: 83.42%\n",
      "Epoch 5, Loss: 0.5338, Train Acc: 89.63%\n",
      "Epoch 6, Loss: 0.4446, Train Acc: 91.08%\n",
      "Epoch 7, Loss: 0.4168, Train Acc: 91.50%\n",
      "Epoch 8, Loss: 0.3980, Train Acc: 91.60%\n",
      "Epoch 9, Loss: 0.3885, Train Acc: 91.78%\n"
     ]
    }
   ],
   "source": [
    "from auto_hyperparameter_tuner import *\n",
    "\n",
    "file_path = \"output/spike_trains_with_stimulus_session_732592105_10.pkl\"\n",
    "best_model_path = \"saved_models/Static_STGAT/best_model_Static_STGAT_715093703_85.21.pth\"\n",
    "\n",
    "trainer = ModelTrainer(param_space)\n",
    "new_mouse_loader, num_nodes, num_classes, spatial_in_features = get_new_mouse_loader(file_path)\n",
    "\n",
    "# Load saved model weights\n",
    "loaded_weights = torch.load(best_model_path)\n",
    "\n",
    "trainer.computed_params['num_classes'] = num_classes\n",
    "trainer.computed_params['spatial_in_features'] = spatial_in_features\n",
    "trainer.computed_params['num_nodes'] = num_nodes\n",
    "trainer.computed_params['lstm_input_dim'] = spatial_in_features * num_nodes\n",
    "trainer.computed_params['temporal_hidden_dim'] = param_space.get('temporal_hidden_dim', 128)\n",
    "trainer.computed_params['temporal_layer_dimension'] = param_space.get('temporal_layer_dimension', 1)\n",
    "trainer.computed_params['num_epochs'] = param_space.get('num_epochs', 10)\n",
    "\n",
    "# Fine-tune model on subset of new mouse data (80/20 split)\n",
    "subset_indices = np.random.choice(len(new_mouse_loader.dataset), size=int(0.8 * len(new_mouse_loader.dataset)), replace=False)\n",
    "subset_dataset = torch.utils.data.Subset(new_mouse_loader.dataset, subset_indices)\n",
    "subset_loader = torch.utils.data.DataLoader(subset_dataset, batch_size=param_space[\"batch_size\"], shuffle=True)\n",
    "\n",
    "X_train = subset_dataset[:][0] \n",
    "\n",
    "# Initialize new model w/ same architecture as loaded model\n",
    "new_model = trainer.initialize_model(param_space, torch.device(\"cuda\"), num_nodes, X_train)\n",
    "\n",
    "# Load weights of matching layers from saved model\n",
    "state_dict = new_model.state_dict()\n",
    "loaded_state_dict = {k: v for k, v in loaded_weights.items() if k in state_dict and state_dict[k].size() == v.size()}\n",
    "state_dict.update(loaded_state_dict)\n",
    "new_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "def transfer_model(model, data_loader, config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_layers = len(list(model.children()))\n",
    "    print(f\"Total number of layers in the model: {num_layers}\")\n",
    "    \n",
    "    #Freeze feature extraction layers \n",
    "    num_layers_to_freeze = 2  \n",
    "    for i, layer in enumerate(model.children()):\n",
    "        if i < num_layers_to_freeze:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Check for parameters w/ requires_grad\n",
    "    params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    if len(params_to_update) > 0:\n",
    "        # Fine-tune only the classification layers\n",
    "        optimizer = Adam(params_to_update, lr=config[\"lr\"])\n",
    "\n",
    "        num_epochs = 5 # Num of fine-tuning epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "\n",
    "            for features, labels in data_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                labels = labels.squeeze().long()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            train_acc = 100 * correct_train / total_train\n",
    "            print(f'Epoch {epoch+1}, Loss: {running_loss / len(data_loader):.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    else:\n",
    "        print(\"No parameters to optimize. Skipping fine-tuning.\")\n",
    "\n",
    "    # Unfreeze the feature extraction layers after fine-tuning (if needed)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Fine-tune \n",
    "transfer_model(new_model, subset_loader, param_space)\n",
    "\n",
    "# Test fine-tuned model on new mouse data\n",
    "trainer.test_loaded_model(new_model, new_mouse_loader, num_nodes, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b498a73-d0ab-40cc-ba77-2af3f1970582",
   "metadata": {},
   "source": [
    "## Train on Mouse 732, test on mouse 715 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c689d5-bf30-4f6a-9c60-53aee4d3d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_hyperparameter_tuner import *\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "param_space = {\n",
    "    \"wandb_project\": \"Predicting Visual Stimulus\",\n",
    "    \"wandb_api_key\": \"7c8d251196fd96d2a93bfb6ffd0005ac030ce42b\",\n",
    "    \"num_epochs\": 50,\n",
    "    \"lr\": 0.000815819,\n",
    "    \"temporal_hidden_dim\": 760,\n",
    "    \"spatial_hidden_dim\": tune.randint(15,20),\n",
    "    \"edge_threshold\":.25,\n",
    "    \"early_stop_patience\": 3,\n",
    "    \"early_stop_delta\": 0.01,\n",
    "    \"batch_size\": 16,\n",
    "    \"graph_batch_size\": 16,\n",
    "    \"temporal_layer_dimension\":1,\n",
    "    \"spatial_out_features\": 1,\n",
    "    \"mouse_number\": 715093703,\n",
    "    \"timesteps\": 10,\n",
    "    \"Architecture\": 'Static_STGAT',\n",
    "    \"num_samples\": 1,\n",
    "    \"accumulation_steps\": 4,  \n",
    "    \"graph_lr\": .05, \n",
    "    \"use_auto_corr_matrix\": True,\n",
    "    \"file_path\":\"/proj/STOR/pipiras/Neuropixel/output/spike_trains_with_stimulus_session_732592105_10.pkl\"\n",
    "    }\n",
    "\n",
    "trainer = ModelTrainer(param_space)\n",
    "trainer.execute_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc13f279-44f8-47a3-91c2-e766d6993314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
