{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnQ0_3PCKiwc",
    "tags": []
   },
   "source": [
    "<a name=\"outline\"></a>\n",
    "# Predicting Visual Perception in Mice with Temporal Neuropixel Data\n",
    "\n",
    ">   The goal of this project is to predict the visual stimulus presented to mice based on the firing network of the synapses between their neurons. We utilize data from the Ecephys Project, which employs Neuropixel technology to record large-scale neural activity. The neural activity, called action potentials, is the a electrical signal generated by the movement of charged ions across the membrane of a neuron. Each unique action potential can also be referred to as a unit. The following analysis includes creating a binned binary spike train dataset from the spike trains given by the Allen Institute, visualizing useful information, and applying deep learning as well as machine learning techniques for image prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "*** Setup the environment. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU is available.\n",
      "Current GPU device: NVIDIA A100-PCIE-40GB MIG 2g.10gb\n",
      "Total RAM: 1006.94 GB\n",
      "Available RAM: 898.76 GB\n"
     ]
    }
   ],
   "source": [
    "# Setup Environment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA GPU is available.\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"CUDA GPU is not available. Using CPU instead.\")\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f\"Current GPU device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "print(f\"Total RAM: {(psutil.virtual_memory().total / (1024**3)):.2f} GB\")\n",
    "print(f\"Available RAM: {(psutil.virtual_memory().available / (1024**3)):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ***Index***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a name=\"step0\"></a>\n",
    "\n",
    "### 0. Load Pre-Processed Data\n",
    "*** Skip if data hasn't been processed. ***\n",
    "\n",
    "    If one has already completed step 1, load the data here. \n",
    "    If not, then complete step 1 first.\n",
    "    \n",
    "- 0.1) Load the dataset with spike times in hertz.\n",
    "- 0.2) Normalize Firing Rates to z scores.\n",
    "- 0.3) Load the pre-normalized data.\n",
    "\n",
    "[Go to Step 0](#step0content)\n",
    "\n",
    "<a name=\"step1\"></a>\n",
    "### 1. Pull and Process Data\n",
    "*** Skip if data is pre-processed. ***\n",
    "\n",
    "    Pull data from the Ecephys repository, process it, then save it to a pickle file.\n",
    "\n",
    "- 1.1) Download and load necessary packages. \n",
    "- 1.2) Create an instance on local computer. \n",
    "- 1.3) Choose session an pull data.\n",
    "- 1.4) Filter invalid spike times.\n",
    "- 1.5) Bin the data. \n",
    "        -1.5.1) Bin the whole experiment.\n",
    "        -1.5.2) Bin by frame in seconds.\n",
    "        -1.5.3) Bin by frame in steps.\n",
    "        -1.5.4) Bin by frame in steps and in parallel. \n",
    "- 1.6) Create frame column and save spike times as a dataframe.\n",
    "- 1.7) Normalize Firing Rates and save normalized firing rates as a dataframe.\n",
    "    \n",
    "\n",
    "[Go to Step 1](#step1content)\n",
    "\n",
    "<a name=\"step2\"></a>\n",
    "### 2. Visualize the Data\n",
    "\n",
    "    Here we will run visualizations to explore the information relevant to predicting visual perception with the data.\n",
    "\n",
    "- 2.0) Pull image data from the Allen SDK Package.\n",
    "- 2.1) All natural scenes presented by their frame number. \n",
    "- 2.2) 2.2) Visualize the unique firing pattern of different single units(usually a single neuron).\n",
    "- 2.3) Raster plot of the spike trains for single units over the course of the natural scene testing. \n",
    "- 2.4) Sorted correlation matrix between single units.\n",
    "- 2.5) Grouped Heat Maps of Unit Resposes by Frames\n",
    "- 2.6) T-SNE Plot All Neurons\n",
    "- 2.7) Average firing rates of units during each frame, in Hertz and by Z scores.\n",
    "\n",
    "[Go to Step 2](#step2content)\n",
    "\n",
    "<a name=\"step3\"></a>\n",
    "### 3. Image Prediction Modeling\n",
    "    \n",
    "    The purpose of this section is to...\n",
    "    1. Employ various machine learning and deep learning techniques to predict the visual stimulus given the data.\n",
    "    2. Produce directed adjacency matrices using a Spatio-Temporal Graph Neural Network. These adjacency matrices may represent the directed functional connectomics between units, usually single neurons, during each frame.\n",
    "\n",
    "- 3.0) Create train splits, test splits, and a correlation based adjacency matrix.\n",
    "- 3.1) Baseline Model: a random guess with an accuracy of 0.85% (1/118).\n",
    "- 3.2) Multiclass Regression\n",
    "- 3.3) Support Vector Machine with Radial Basis Function.\n",
    "- 3.4) Principal Component Regression.\n",
    "- 3.5) Neural Network with one hidden layer.\n",
    "- 3.6) Deep Neural Network\n",
    "- 3.7) Long-Short Term Memory model.\n",
    "- 3.8) Static Graph Neural Network.\n",
    "- 3.9) Graph Attention Network\n",
    "- 3.10) static ST-GNN.\n",
    "- 3.11) Dynamic STGNN with adjacency matrix for each frame.\n",
    "- 3.12) Dynamic STGNN with adjacency matrix for each timestep and frame.\n",
    "\n",
    "[Go to Step 3](#step3content)\n",
    "\n",
    "<a name=\"step4\"></a>\n",
    "### 4. Modelling Outcomes\n",
    "    The purpose of 4 is to create a table that allows one to compare each model and their parameters side by side and visulize the found network.\n",
    "\n",
    "- 4.1) Graph Model Metrics\n",
    "- 4.2) Visualize Found Graph Network\n",
    "[Go to Step 4](#step4content)\n",
    "\n",
    "## References\n",
    "[Go to References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u8gxzbyLKTm",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a name=\"step0content\"></a>\n",
    "## 0. Load Pre-Processed Data\n",
    "    Only if step 1 has already been completed.\n",
    "[Go to Outline](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j8xsxH_kzQ4"
   },
   "source": [
    "*** Skip if data hasn't been processed yet. *** \\\n",
    "Run step 0 to load the pre-processed data. Step 1 takes some processing time and it saves the processed data into files that can later be loaded through this step.\n",
    "\n",
    "- 0.1) Load the dataset with spike times in hertz.\n",
    "\n",
    "- 0.2) 0.2) Normalize Firing Rates to z scores.\n",
    "\n",
    "- 0.3) Load the pre-normalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExcZz5n4lcWT",
    "tags": []
   },
   "source": [
    "### 0.1) Load the dataset with spike times in hertz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3502,
     "status": "ok",
     "timestamp": 1684799143509,
     "user": {
      "displayName": "Raymond Carpenter",
      "userId": "11372441929354406810"
     },
     "user_tz": 240
    },
    "id": "uSVw-0xE0fi-",
    "outputId": "144e19f8-18d8-463f-d9eb-2e3693cdd9ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>950907203</th>\n",
       "      <th>950907205</th>\n",
       "      <th>950907518</th>\n",
       "      <th>950907524</th>\n",
       "      <th>950907209</th>\n",
       "      <th>950907207</th>\n",
       "      <th>950907528</th>\n",
       "      <th>950907526</th>\n",
       "      <th>950907214</th>\n",
       "      <th>950907216</th>\n",
       "      <th>...</th>\n",
       "      <th>950913428</th>\n",
       "      <th>950913495</th>\n",
       "      <th>950913484</th>\n",
       "      <th>950913475</th>\n",
       "      <th>950913466</th>\n",
       "      <th>950913839</th>\n",
       "      <th>950913553</th>\n",
       "      <th>950913542</th>\n",
       "      <th>950913529</th>\n",
       "      <th>frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59496</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59497</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59498</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59500 rows × 1192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       950907203  950907205  950907518  950907524  950907209  950907207  \\\n",
       "0              0          2          0          0          0          0   \n",
       "1              0          1          0          0          0          0   \n",
       "2              0          0          0          0          0          0   \n",
       "3              0          0          0          0          0          0   \n",
       "4              0          1          0          0          0          0   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "59495          0          0          0          0          0          0   \n",
       "59496          0          0          0          0          0          0   \n",
       "59497          0          1          0          0          0          0   \n",
       "59498          0          2          0          0          0          0   \n",
       "59499          0          0          0          0          0          0   \n",
       "\n",
       "       950907528  950907526  950907214  950907216  ...  950913428  950913495  \\\n",
       "0              0          0          0          0  ...          0          0   \n",
       "1              0          1          0          0  ...          0          0   \n",
       "2              0          0          0          0  ...          0          0   \n",
       "3              0          0          0          0  ...          0          0   \n",
       "4              0          1          1          0  ...          0          0   \n",
       "...          ...        ...        ...        ...  ...        ...        ...   \n",
       "59495          0          0          1          0  ...          0          0   \n",
       "59496          0          0          1          0  ...          0          0   \n",
       "59497          0          0          0          0  ...          0          0   \n",
       "59498          0          0          0          0  ...          0          0   \n",
       "59499          0          0          0          0  ...          0          0   \n",
       "\n",
       "       950913484  950913475  950913466  950913839  950913553  950913542  \\\n",
       "0              0          0          0          0          1          0   \n",
       "1              0          1          0          0          0          0   \n",
       "2              0          1          0          0          0          0   \n",
       "3              0          0          0          0          0          0   \n",
       "4              0          0          0          0          0          0   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "59495          0          0          0          0          1          0   \n",
       "59496          0          0          0          0          1          0   \n",
       "59497          0          0          0          0          0          0   \n",
       "59498          0          1          0          0          0          0   \n",
       "59499          0          0          0          0          0          0   \n",
       "\n",
       "       950913529  frame  \n",
       "0              0   92.0  \n",
       "1              0   92.0  \n",
       "2              0   92.0  \n",
       "3              0   92.0  \n",
       "4              0   92.0  \n",
       "...          ...    ...  \n",
       "59495          0   17.0  \n",
       "59496          0   17.0  \n",
       "59497          0   17.0  \n",
       "59498          0   17.0  \n",
       "59499          0   17.0  \n",
       "\n",
       "[59500 rows x 1192 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dictionary of valid spike times from the pickle.\n",
    "with open('spike_trains_with_stimulus_session_721123822.pkl', 'rb') as f:\n",
    "    spike_df = pickle.load(f)\n",
    "spike_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQUalmkolwzs",
    "tags": []
   },
   "source": [
    "### 0.2) Normalize Firing Rates to z scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5860,
     "status": "ok",
     "timestamp": 1684798127533,
     "user": {
      "displayName": "Raymond Carpenter",
      "userId": "11372441929354406810"
     },
     "user_tz": 240
    },
    "id": "7EChqWOblwTz",
    "outputId": "3a031af6-66f6-4fc8-e333-8ae279ef8281"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>950907205</th>\n",
       "      <th>950907518</th>\n",
       "      <th>950907524</th>\n",
       "      <th>950907209</th>\n",
       "      <th>950907528</th>\n",
       "      <th>950907526</th>\n",
       "      <th>950907214</th>\n",
       "      <th>950907216</th>\n",
       "      <th>950907211</th>\n",
       "      <th>...</th>\n",
       "      <th>950913437</th>\n",
       "      <th>950913428</th>\n",
       "      <th>950913495</th>\n",
       "      <th>950913484</th>\n",
       "      <th>950913475</th>\n",
       "      <th>950913466</th>\n",
       "      <th>950913839</th>\n",
       "      <th>950913553</th>\n",
       "      <th>950913542</th>\n",
       "      <th>950913529</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.0</td>\n",
       "      <td>3.147226</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>1.177539</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.0</td>\n",
       "      <td>1.224244</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>7.736807</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>2.633954</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.0</td>\n",
       "      <td>-0.698737</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>2.633954</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92.0</td>\n",
       "      <td>-0.698737</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92.0</td>\n",
       "      <td>1.224244</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>7.736807</td>\n",
       "      <td>1.529261</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59495</th>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.698737</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>1.529261</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>1.177539</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59496</th>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.698737</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>1.529261</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>1.177539</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59497</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.224244</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59498</th>\n",
       "      <td>17.0</td>\n",
       "      <td>3.147226</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>2.633954</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59499</th>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.698737</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59000 rows × 1172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      frame  950907205  950907518  950907524  950907209  950907528  950907526  \\\n",
       "0      92.0   3.147226  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "1      92.0   1.224244  -0.030984  -0.211255  -0.032035  -0.010042   7.736807   \n",
       "2      92.0  -0.698737  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "3      92.0  -0.698737  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "4      92.0   1.224244  -0.030984  -0.211255  -0.032035  -0.010042   7.736807   \n",
       "...     ...        ...        ...        ...        ...        ...        ...   \n",
       "59495  17.0  -0.698737  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "59496  17.0  -0.698737  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "59497  17.0   1.224244  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "59498  17.0   3.147226  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "59499  17.0  -0.698737  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "\n",
       "       950907214  950907216  950907211  ...  950913437  950913428  950913495  \\\n",
       "0      -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "1      -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "2      -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "3      -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "4       1.529261  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "...          ...        ...        ...  ...        ...        ...        ...   \n",
       "59495   1.529261  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "59496   1.529261  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "59497  -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "59498  -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "59499  -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "\n",
       "       950913484  950913475  950913466  950913839  950913553  950913542  \\\n",
       "0      -0.160753  -0.279658  -0.206547  -0.056136   1.177539  -0.211908   \n",
       "1      -0.160753   2.633954  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "2      -0.160753   2.633954  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "3      -0.160753  -0.279658  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "4      -0.160753  -0.279658  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "59495  -0.160753  -0.279658  -0.206547  -0.056136   1.177539  -0.211908   \n",
       "59496  -0.160753  -0.279658  -0.206547  -0.056136   1.177539  -0.211908   \n",
       "59497  -0.160753  -0.279658  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "59498  -0.160753   2.633954  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "59499  -0.160753  -0.279658  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "\n",
       "       950913529  \n",
       "0       -0.11824  \n",
       "1       -0.11824  \n",
       "2       -0.11824  \n",
       "3       -0.11824  \n",
       "4       -0.11824  \n",
       "...          ...  \n",
       "59495   -0.11824  \n",
       "59496   -0.11824  \n",
       "59497   -0.11824  \n",
       "59498   -0.11824  \n",
       "59499   -0.11824  \n",
       "\n",
       "[59000 rows x 1172 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Clean the average firing rates by normalizing the data and removing outliers.\n",
    "# The highest_value removes neurons who have a z score above that value.\n",
    "# The lowest_value removes neurons whose z score never exceeds that value.\n",
    "def clean_avg_firing_rates(df, highest_value, lowest_value):\n",
    "    dfdataframe = df.copy()\n",
    "    dfdataframe = dfdataframe.drop('frame', axis = 1)\n",
    "    \n",
    "    # Normalize the firing rates by calculating z-scores\n",
    "    normalized_firing_rates = (dfdataframe.iloc[:, 1:] - dfdataframe.iloc[:, 1:].mean()) / dfdataframe.iloc[:, 1:].std()\n",
    "\n",
    "    # Add the 'frame' column back to the normalized DataFrame\n",
    "    normalized_firing_rates.insert(0, 'frame', df['frame'])\n",
    "\n",
    "    # Identify the neurons that meet the given criteria\n",
    "    selected_neurons_mask = (~(normalized_firing_rates.iloc[:, 1:] > highest_value).any(axis=0)) & ((normalized_firing_rates.iloc[:, 1:] > lowest_value).any(axis=0))\n",
    "\n",
    "    # Filter the DataFrame based on the selected neurons\n",
    "    filtered_normalized_firing_rates = normalized_firing_rates.loc[:, ['frame'] + selected_neurons_mask[selected_neurons_mask].index.tolist()]\n",
    "    \n",
    "    return filtered_normalized_firing_rates\n",
    "\n",
    "# Use the function with the desired parameters\n",
    "filtered_normalized_firing_rates = clean_avg_firing_rates(spike_df, highest_value=1000000000000000, lowest_value=-10)\n",
    "\n",
    "# Drop rows where the frame is -1.\n",
    "filtered_normalized_firing_rates = filtered_normalized_firing_rates[filtered_normalized_firing_rates['frame'] != -1]\n",
    "\n",
    "filtered_normalized_firing_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeZm8KzqmKm6",
    "tags": []
   },
   "source": [
    "### 0.3) Load the pre-normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 1534,
     "status": "ok",
     "timestamp": 1684811212000,
     "user": {
      "displayName": "Raymond Carpenter",
      "userId": "11372441929354406810"
     },
     "user_tz": 240
    },
    "id": "yI14wTl7mEYh",
    "outputId": "3ad5faef-3d5a-43af-b788-5f849d557048",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>950907205</th>\n",
       "      <th>950907518</th>\n",
       "      <th>950907524</th>\n",
       "      <th>950907209</th>\n",
       "      <th>950907528</th>\n",
       "      <th>950907526</th>\n",
       "      <th>950907214</th>\n",
       "      <th>950907216</th>\n",
       "      <th>950907211</th>\n",
       "      <th>...</th>\n",
       "      <th>950913437</th>\n",
       "      <th>950913428</th>\n",
       "      <th>950913495</th>\n",
       "      <th>950913484</th>\n",
       "      <th>950913475</th>\n",
       "      <th>950913466</th>\n",
       "      <th>950913839</th>\n",
       "      <th>950913553</th>\n",
       "      <th>950913542</th>\n",
       "      <th>950913529</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.0</td>\n",
       "      <td>3.147226</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>1.177539</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.0</td>\n",
       "      <td>1.224244</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>7.736807</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>2.633954</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.0</td>\n",
       "      <td>-0.698737</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>2.633954</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92.0</td>\n",
       "      <td>-0.698737</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92.0</td>\n",
       "      <td>1.224244</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>7.736807</td>\n",
       "      <td>1.529261</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59495</th>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.698737</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>1.529261</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>1.177539</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59496</th>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.698737</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>1.529261</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>1.177539</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59497</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.224244</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59498</th>\n",
       "      <td>17.0</td>\n",
       "      <td>3.147226</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>2.633954</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59499</th>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.698737</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-0.211255</td>\n",
       "      <td>-0.032035</td>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.110522</td>\n",
       "      <td>-0.560325</td>\n",
       "      <td>-0.468267</td>\n",
       "      <td>-0.02426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.160645</td>\n",
       "      <td>-0.345801</td>\n",
       "      <td>-0.160753</td>\n",
       "      <td>-0.279658</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>-0.056136</td>\n",
       "      <td>-0.617444</td>\n",
       "      <td>-0.211908</td>\n",
       "      <td>-0.11824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59500 rows × 1142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      frame  950907205  950907518  950907524  950907209  950907528  950907526  \\\n",
       "0      92.0   3.147226  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "1      92.0   1.224244  -0.030984  -0.211255  -0.032035  -0.010042   7.736807   \n",
       "2      92.0  -0.698737  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "3      92.0  -0.698737  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "4      92.0   1.224244  -0.030984  -0.211255  -0.032035  -0.010042   7.736807   \n",
       "...     ...        ...        ...        ...        ...        ...        ...   \n",
       "59495  17.0  -0.698737  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "59496  17.0  -0.698737  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "59497  17.0   1.224244  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "59498  17.0   3.147226  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "59499  17.0  -0.698737  -0.030984  -0.211255  -0.032035  -0.010042  -0.110522   \n",
       "\n",
       "       950907214  950907216  950907211  ...  950913437  950913428  950913495  \\\n",
       "0      -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "1      -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "2      -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "3      -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "4       1.529261  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "...          ...        ...        ...  ...        ...        ...        ...   \n",
       "59495   1.529261  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "59496   1.529261  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "59497  -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "59498  -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "59499  -0.560325  -0.468267   -0.02426  ...  -0.221451  -0.160645  -0.345801   \n",
       "\n",
       "       950913484  950913475  950913466  950913839  950913553  950913542  \\\n",
       "0      -0.160753  -0.279658  -0.206547  -0.056136   1.177539  -0.211908   \n",
       "1      -0.160753   2.633954  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "2      -0.160753   2.633954  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "3      -0.160753  -0.279658  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "4      -0.160753  -0.279658  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "59495  -0.160753  -0.279658  -0.206547  -0.056136   1.177539  -0.211908   \n",
       "59496  -0.160753  -0.279658  -0.206547  -0.056136   1.177539  -0.211908   \n",
       "59497  -0.160753  -0.279658  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "59498  -0.160753   2.633954  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "59499  -0.160753  -0.279658  -0.206547  -0.056136  -0.617444  -0.211908   \n",
       "\n",
       "       950913529  \n",
       "0       -0.11824  \n",
       "1       -0.11824  \n",
       "2       -0.11824  \n",
       "3       -0.11824  \n",
       "4       -0.11824  \n",
       "...          ...  \n",
       "59495   -0.11824  \n",
       "59496   -0.11824  \n",
       "59497   -0.11824  \n",
       "59498   -0.11824  \n",
       "59499   -0.11824  \n",
       "\n",
       "[59500 rows x 1142 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_normalized_pickle = 'filtered_normalized_pickle_721123822.pkl'\n",
    "\n",
    "# Load the dictionary of valid spike times from the pickle.\n",
    "with open(filtered_normalized_pickle, 'rb') as f:\n",
    "    filtered_normalized_firing_rates = pickle.load(f)\n",
    "    \n",
    "filtered_normalized_firing_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MVUgffQlNCq",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 0.x) Set path to file in one's Google Drive. (Used for google drive saves only.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1685285010825,
     "user": {
      "displayName": "Raywzhere",
      "userId": "17705554742553511920"
     },
     "user_tz": 240
    },
    "id": "KR0s2zhenWcB",
    "outputId": "d41885c6-a8d3-4548-bcff-6cda2e230aba"
   },
   "outputs": [],
   "source": [
    "# Path to pickle file in one's Google Drive.\n",
    "pickle_file_path = '/content/drive/MyDrive/research/spike_trains_with_stimulus_session_721123822.pkl'\n",
    "\n",
    "# Mount google drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQmuQKHALKa3",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a name=\"step1content\"></a>\n",
    "## 1. Pull and Process Data\n",
    "[Go to Outline](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WkkIuYjjh12"
   },
   "source": [
    "*** Skip if data is pre-processed. *** \\\n",
    "The purpose of step one is to pull the necessary data for the project from the AllenSDK package, process the data, and save them.\n",
    "\n",
    "- 1.1) Download and load necessary packages. \n",
    "- 1.2) Create an instance on local computer. \n",
    "- 1.3) Choose session an pull data.\n",
    "- 1.4) Filter invalid spike times.\n",
    "- 1.5) Bin the data. \n",
    "        -1.5.1) Bin the whole experiment.\n",
    "        -1.5.2) Bin by frame in seconds.\n",
    "        -1.5.3) Bin by frame in steps.\n",
    "        -1.5.4) Bin by frame in steps and in parallel. \n",
    "- 1.6) Create frame column and save spike times as a dataframe.\n",
    "- 1.7) Normalize Firing Rates and save normalized firing rates as a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bbQB_WBgPQG",
    "tags": []
   },
   "source": [
    "### 1.1) Download and load necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 32865,
     "status": "ok",
     "timestamp": 1685285043688,
     "user": {
      "displayName": "Raywzhere",
      "userId": "17705554742553511920"
     },
     "user_tz": 240
    },
    "id": "fyzjxYrlVxIi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load packages needed for step 1.\n",
    "!pip install -q allensdk\n",
    "\n",
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQ0IJMbEgPrb",
    "tags": []
   },
   "source": [
    "### 1.2) Create an instance on local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3954,
     "status": "ok",
     "timestamp": 1685285047632,
     "user": {
      "displayName": "Raywzhere",
      "userId": "17705554742553511920"
     },
     "user_tz": 240
    },
    "id": "aoPTGepefx5_",
    "outputId": "00f8ef50-0aae-4477-db98-16572d946bfb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing manifest.json file.\n",
      "Session keys:\n",
      "[715093703, 719161530, 721123822, 732592105, 737581020, 739448407, 742951821, 743475441, 744228101, 746083955, 750332458, 750749662, 751348571, 754312389, 754829445, 755434585, 756029989, 757216464, 757970808, 758798717, 759883607, 760345702, 760693773, 761418226, 762120172, 762602078, 763673393, 766640955, 767871931, 768515987, 771160300, 771990200, 773418906, 774875821, 778240327, 778998620, 779839471, 781842082, 786091066, 787025148, 789848216, 791319847, 793224716, 794812542, 797828357, 798911424, 799864342, 816200189, 819186360, 819701982, 821695405, 829720705, 831882777, 835479236, 839068429, 839557629, 840012044, 847657808]\n"
     ]
    }
   ],
   "source": [
    "# Set output directory to a new folder called 'output' in the current working directory\n",
    "output_dir = os.path.join(os.getcwd(), 'output')\n",
    "\n",
    "# Check if the output directory exists, and create it if it doesn't\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Set DOWNLOAD_COMPLETE_DATASET to True\n",
    "DOWNLOAD_COMPLETE_DATASET = True\n",
    "\n",
    "# Create a file path to the manifest.json file within the output directory\n",
    "manifest_path = os.path.join(output_dir, \"manifest.json\")\n",
    "\n",
    "# Check if the manifest.json file exists\n",
    "if os.path.exists(manifest_path):\n",
    "    print(\"Using existing manifest.json file.\")\n",
    "else:\n",
    "    print(\"Creating a new manifest.json file.\")\n",
    "\n",
    "# Create an instance of the EcephysProjectCache class with the manifest file path as argument\n",
    "cache = EcephysProjectCache(manifest=manifest_path)\n",
    "# Get session table\n",
    "session_table = cache.get_session_table()\n",
    "\n",
    "# Display session keys\n",
    "session_keys = []\n",
    "print(\"Session keys:\")\n",
    "for session_key in session_table.index:\n",
    "    session_keys.append(session_key)\n",
    "print(session_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x86qhjr_gp7k",
    "tags": []
   },
   "source": [
    "### 1.3) Choose session and pull data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1685285047632,
     "user": {
      "displayName": "Raywzhere",
      "userId": "17705554742553511920"
     },
     "user_tz": 240
    },
    "id": "ytgWvBo3gnRB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    " Pick session number\n",
    " '''\n",
    " \n",
    "session_number = 721123822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59418,
     "status": "ok",
     "timestamp": 1685285107048,
     "user": {
      "displayName": "Raywzhere",
      "userId": "17705554742553511920"
     },
     "user_tz": 240
    },
    "id": "O_z4tv8LfyDj",
    "outputId": "54af6a79-e744-4e76-a019-4b296e487d99",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/rayrayc/.local/lib/python3.9/site-packages/allensdk/brain_observatory/ecephys/ecephys_session.py:1371: UserWarning: Session includes invalid time intervals that could be accessed with the attribute 'invalid_times',Spikes within these intervals are invalid and may need to be excluded from the analysis.\n",
      "  warnings.warn(\"Session includes invalid time intervals that could \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session objects\n",
      "['DETAILED_STIMULUS_PARAMETERS', 'LazyProperty', 'age_in_days', 'api', 'channel_structure_intervals', 'channels', 'conditionwise_spike_statistics', 'ecephys_session_id', 'from_nwb_path', 'full_genotype', 'get_current_source_density', 'get_inter_presentation_intervals_for_stimulus', 'get_invalid_times', 'get_lfp', 'get_parameter_values_for_stimulus', 'get_pupil_data', 'get_screen_gaze_data', 'get_stimulus_epochs', 'get_stimulus_parameter_values', 'get_stimulus_table', 'inter_presentation_intervals', 'invalid_times', 'mean_waveforms', 'metadata', 'num_channels', 'num_probes', 'num_stimulus_presentations', 'num_units', 'optogenetic_stimulation_epochs', 'presentationwise_spike_counts', 'presentationwise_spike_times', 'probes', 'rig_equipment_name', 'rig_geometry_data', 'running_speed', 'session_start_time', 'session_type', 'sex', 'specimen_name', 'spike_amplitudes', 'spike_times', 'stimulus_conditions', 'stimulus_names', 'stimulus_presentations', 'structure_acronyms', 'structurewise_unit_counts', 'units']\n"
     ]
    }
   ],
   "source": [
    "# Pull session.\n",
    "session = cache.get_session_data(session_number,\n",
    "                                 isi_violations_maximum = np.inf,\n",
    "                                 amplitude_cutoff_maximum = np.inf,\n",
    "                                 presence_ratio_minimum = -np.inf\n",
    "                                )\n",
    "# Get spike times.\n",
    "spike_times = session.spike_times\n",
    "# Get specific stimulus table.\n",
    "stimulus_table = session.get_stimulus_table(\"natural_scenes\")\n",
    "\n",
    "# Display objects within session.\n",
    "print(\"Session objects\")\n",
    "print([attr_or_method for attr_or_method in dir(session) if attr_or_method[0] != '_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pXhgHnng0bL",
    "tags": []
   },
   "source": [
    "### 1.4) Filter invalid spike times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1685285107971,
     "user": {
      "displayName": "Raywzhere",
      "userId": "17705554742553511920"
     },
     "user_tz": 240
    },
    "id": "X4VRUgB0gx1M",
    "outputId": "c90b1269-1308-49df-ca48-f25ee7c42d56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering valid spike times: 100%|██████████| 1191/1191 [00:00<00:00, 1670.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Access the invalid_times DataFrame\n",
    "invalid_times = session.invalid_times\n",
    "\n",
    "# Function to check if a spike time is valid\n",
    "def is_valid_time(spike_times, invalid_intervals):\n",
    "    invalid = np.zeros_like(spike_times, dtype=bool)\n",
    "    for _, row in invalid_intervals.iterrows():\n",
    "        start, end = row['start_time'], row['stop_time']\n",
    "        invalid |= (spike_times >= start) & (spike_times <= end)\n",
    "    return ~invalid\n",
    "\n",
    "# Filter the valid spike times\n",
    "valid_spike_times = {}\n",
    "with tqdm(total=len(spike_times), desc='Filtering valid spike times') as pbar:\n",
    "    for neuron, times in spike_times.items():\n",
    "        valid_mask = is_valid_time(times, session.invalid_times)\n",
    "        valid_spike_times[neuron] = times[valid_mask]\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIPjYshk9eTQ",
    "tags": []
   },
   "source": [
    "### 1.5) Bin the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-AKtrRqlo91",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1.5.1) Bin the whole experiment. \n",
    "With respect to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qR64_wlD9spS"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set the bin_size below.\n",
    "\n",
    "A \"bin_size = .01\" will create a 10 millisecond per timestep dataframe.\n",
    "While a \"bin_size = 1\" will create a 1 second per timestep dataframe.\n",
    "'''\n",
    "bin_size = .025\n",
    "\n",
    "### Bin the data based on the bin_size.\n",
    "\n",
    "# Where each column is a unique neuron and each row is a binned timeframe.\n",
    "spike_times = valid_spike_times\n",
    "# Parameters\n",
    "num_neurons = len(spike_times.keys())\n",
    "\n",
    "# Find the maximum time across all neurons\n",
    "max_time = max([max(times) for times in spike_times.values() if len(times) > 0])\n",
    "\n",
    "# Calculate the number of bins\n",
    "num_bins = int(np.ceil(max_time / bin_size))\n",
    "\n",
    "# Create an empty binary spike matrix\n",
    "spike_matrix = np.zeros((num_neurons, num_bins), dtype=np.int32)\n",
    "\n",
    "# Bin the spike times and fill the spike_matrix\n",
    "with tqdm(total=num_neurons, desc='Processing neurons') as pbar:\n",
    "    for neuron_idx, times in enumerate(spike_times.values()):\n",
    "\n",
    "        # Convert spike times to bin indices\n",
    "        bin_indices = (times // bin_size).astype(int)\n",
    "\n",
    "        # Update the corresponding bins\n",
    "        for bin_index in bin_indices:\n",
    "            spike_matrix[neuron_idx, bin_index] += 1\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "# Convert the spike matrix to a pandas DataFrame and set the index to neuron IDs\n",
    "spike_dataframe = pd.DataFrame(spike_matrix, index=spike_times.keys())\n",
    "\n",
    "spike_dataframe.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjcPAAR1lo91",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1.5.2) Bin with respect to each frame by seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "executionInfo": {
     "elapsed": 5743525,
     "status": "ok",
     "timestamp": 1685296580002,
     "user": {
      "displayName": "Raywzhere",
      "userId": "17705554742553511920"
     },
     "user_tz": 240
    },
    "id": "R_g8T18s5b92",
    "outputId": "4e55c8fc-9a11-44a0-a961-abec94a344f2"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "bin_size = 0.025  # Set bin size (in seconds)\n",
    "\n",
    "stimulus_table = session.get_stimulus_table(\"natural_scenes\")\n",
    "\n",
    "# The start times of each stimulus presentation\n",
    "image_start_times = stimulus_table.start_time.values\n",
    "\n",
    "# The end times of each stimulus presentation\n",
    "image_end_times = stimulus_table.stop_time.values  \n",
    "\n",
    "# The number of bins per image presentation\n",
    "bins_per_image = np.ceil((image_end_times - image_start_times) / bin_size).astype(int)\n",
    "\n",
    "# The total number of bins\n",
    "total_bins = bins_per_image.sum()\n",
    "\n",
    "# Create an empty binary spike matrix\n",
    "num_neurons = len(spike_times.keys())\n",
    "spike_matrix = np.zeros((num_neurons, total_bins), dtype=np.int32)\n",
    "\n",
    "with tqdm(total=num_neurons, desc='Processing neurons') as pbar:\n",
    "    for neuron_idx, times in enumerate(spike_times.values()):\n",
    "        # The start bin for the next image presentation\n",
    "        start_bin = 0  # Move this line inside the loop over neuron_idx\n",
    "        \n",
    "        \n",
    "        for image_idx, (start_time, end_time) in enumerate(zip(image_start_times, image_end_times)):\n",
    "            print(times)\n",
    "            # Bin edges for this image presentation\n",
    "            bin_edges = np.linspace(start_time, end_time, bins_per_image[image_idx] + 1)\n",
    "\n",
    "            # Bin the spike times for this image presentation\n",
    "            binned_spike_times = np.histogram(times, bins=bin_edges)[0]\n",
    "\n",
    "            # Add the binned spike times to the spike matrix\n",
    "            end_bin = start_bin + bins_per_image[image_idx]\n",
    "            if len(binned_spike_times) == len(spike_matrix[neuron_idx, start_bin:end_bin]):\n",
    "                spike_matrix[neuron_idx, start_bin:end_bin] = binned_spike_times\n",
    "\n",
    "            # Update the start bin for the next image presentation\n",
    "            start_bin = end_bin\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "# Convert the spike matrix to a pandas DataFrame and set the index to neuron IDs\n",
    "spike_dataframe = pd.DataFrame(spike_matrix, index=spike_times.keys())\n",
    "\n",
    "spike_dataframe.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1.5.3) Bin with respect to each frame by steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "timesteps_per_frame = 10  # Set the number of timesteps per frame\n",
    "\n",
    "# Stimulus table.\n",
    "stimulus_table = session.get_stimulus_table(\"natural_scenes\")\n",
    "\n",
    "# The start times of each stimulus presentation\n",
    "image_start_times = torch.tensor(stimulus_table.start_time.values)\n",
    "\n",
    "# The end times of each stimulus presentation\n",
    "image_end_times = torch.tensor(stimulus_table.stop_time.values)\n",
    "\n",
    "# The duration of each image presentation\n",
    "image_durations = image_end_times - image_start_times\n",
    "\n",
    "# The bin size for each image presentation\n",
    "bin_sizes = image_durations / timesteps_per_frame\n",
    "\n",
    "# The number of bins per image presentation\n",
    "bins_per_image = timesteps_per_frame\n",
    "\n",
    "# The total number of bins\n",
    "total_bins = bins_per_image * len(image_start_times)\n",
    "\n",
    "# Create an empty binary spike matrix\n",
    "num_neurons = len(spike_times.keys())\n",
    "spike_matrix = torch.zeros((num_neurons, total_bins), dtype=torch.int32)\n",
    "\n",
    "with tqdm(total=num_neurons, desc='Processing neurons') as pbar:\n",
    "    for neuron_idx, times in enumerate(spike_times.values()):\n",
    "        # The start bin for the next image presentation\n",
    "        start_bin = 0  # Move this line inside the loop over neuron_idx\n",
    "        for image_idx, (start_time, end_time) in enumerate(zip(image_start_times, image_end_times)):\n",
    "            # Bin edges for this image presentation\n",
    "            bin_edges = torch.linspace(start_time, end_time, bins_per_image + 1)\n",
    "\n",
    "            # Bin the spike times for this image presentation\n",
    "            binned_spike_times = torch.histc(torch.tensor(times), bins=bin_edges.shape[0]-1, min=bin_edges.min(), max=bin_edges.max())\n",
    "\n",
    "            # Add the binned spike times to the spike matrix\n",
    "            end_bin = start_bin + bins_per_image\n",
    "            if len(binned_spike_times) == len(spike_matrix[neuron_idx, start_bin:end_bin]):\n",
    "                spike_matrix[neuron_idx, start_bin:end_bin] = binned_spike_times\n",
    "\n",
    "            # Update the start bin for the next image presentation\n",
    "            start_bin = end_bin\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "# Convert the spike matrix to a pandas DataFrame and set the index to neuron IDs\n",
    "spike_dataframe = pd.DataFrame(spike_matrix.numpy(), index=spike_times.keys())\n",
    "\n",
    "spike_dataframe.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.5.4) Split by frame and in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing neurons: 100%|██████████| 1191/1191 [03:22<00:00,  5.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>950907203</th>\n",
       "      <th>950907205</th>\n",
       "      <th>950907518</th>\n",
       "      <th>950907524</th>\n",
       "      <th>950907209</th>\n",
       "      <th>950907207</th>\n",
       "      <th>950907528</th>\n",
       "      <th>950907526</th>\n",
       "      <th>950907214</th>\n",
       "      <th>950907216</th>\n",
       "      <th>...</th>\n",
       "      <th>950913437</th>\n",
       "      <th>950913428</th>\n",
       "      <th>950913495</th>\n",
       "      <th>950913484</th>\n",
       "      <th>950913475</th>\n",
       "      <th>950913466</th>\n",
       "      <th>950913839</th>\n",
       "      <th>950913553</th>\n",
       "      <th>950913542</th>\n",
       "      <th>950913529</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59496</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59497</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59498</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59500 rows × 1191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       950907203  950907205  950907518  950907524  950907209  950907207  \\\n",
       "0              0          2          0          0          0          0   \n",
       "1              0          1          0          0          0          0   \n",
       "2              0          0          0          0          0          0   \n",
       "3              0          0          0          0          0          0   \n",
       "4              0          1          0          0          0          0   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "59495          0          0          0          0          0          0   \n",
       "59496          0          0          0          0          0          0   \n",
       "59497          0          1          0          0          0          0   \n",
       "59498          0          2          0          0          0          0   \n",
       "59499          0          0          0          0          0          0   \n",
       "\n",
       "       950907528  950907526  950907214  950907216  ...  950913437  950913428  \\\n",
       "0              0          0          0          0  ...          0          0   \n",
       "1              0          1          0          0  ...          0          0   \n",
       "2              0          0          0          0  ...          0          0   \n",
       "3              0          0          0          0  ...          0          0   \n",
       "4              0          1          1          0  ...          0          0   \n",
       "...          ...        ...        ...        ...  ...        ...        ...   \n",
       "59495          0          0          1          0  ...          0          0   \n",
       "59496          0          0          1          0  ...          0          0   \n",
       "59497          0          0          0          0  ...          0          0   \n",
       "59498          0          0          0          0  ...          0          0   \n",
       "59499          0          0          0          0  ...          0          0   \n",
       "\n",
       "       950913495  950913484  950913475  950913466  950913839  950913553  \\\n",
       "0              0          0          0          0          0          1   \n",
       "1              0          0          1          0          0          0   \n",
       "2              0          0          1          0          0          0   \n",
       "3              0          0          0          0          0          0   \n",
       "4              0          0          0          0          0          0   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "59495          0          0          0          0          0          1   \n",
       "59496          0          0          0          0          0          1   \n",
       "59497          0          0          0          0          0          0   \n",
       "59498          0          0          1          0          0          0   \n",
       "59499          0          0          0          0          0          0   \n",
       "\n",
       "       950913542  950913529  \n",
       "0              0          0  \n",
       "1              0          0  \n",
       "2              0          0  \n",
       "3              0          0  \n",
       "4              0          0  \n",
       "...          ...        ...  \n",
       "59495          0          0  \n",
       "59496          0          0  \n",
       "59497          0          0  \n",
       "59498          0          0  \n",
       "59499          0          0  \n",
       "\n",
       "[59500 rows x 1191 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Parameters\n",
    "timesteps_per_frame = 10  # Set the number of timesteps per frame\n",
    "\n",
    "# Stimulus table.\n",
    "stimulus_table = session.get_stimulus_table(\"natural_scenes\")\n",
    "\n",
    "# The start times of each stimulus presentation\n",
    "image_start_times = torch.tensor(stimulus_table.start_time.values)\n",
    "\n",
    "# The end times of each stimulus presentation\n",
    "image_end_times = torch.tensor(stimulus_table.stop_time.values)\n",
    "\n",
    "# The duration of each image presentation\n",
    "image_durations = image_end_times - image_start_times\n",
    "\n",
    "# The bin size for each image presentation\n",
    "bin_sizes = image_durations / timesteps_per_frame\n",
    "\n",
    "# The number of bins per image presentation\n",
    "bins_per_image = timesteps_per_frame\n",
    "\n",
    "# The total number of bins\n",
    "total_bins = bins_per_image * len(image_start_times)\n",
    "\n",
    "# Create an empty binary spike matrix\n",
    "num_neurons = len(spike_times.keys())\n",
    "\n",
    "def process_neuron(times):\n",
    "    # The start bin for the next image presentation\n",
    "    start_bin = 0\n",
    "    neuron_spike_bins = torch.zeros(total_bins, dtype=torch.int32)\n",
    "    for image_idx, (start_time, end_time) in enumerate(zip(image_start_times, image_end_times)):\n",
    "        # Bin edges for this image presentation\n",
    "        bin_edges = torch.linspace(start_time, end_time, bins_per_image + 1)\n",
    "\n",
    "        # Bin the spike times for this image presentation\n",
    "        binned_spike_times = torch.histc(torch.tensor(times), bins=bin_edges.shape[0]-1, min=bin_edges.min(), max=bin_edges.max())\n",
    "\n",
    "        # Add the binned spike times to the spike matrix\n",
    "        end_bin = start_bin + bins_per_image\n",
    "        if len(binned_spike_times) == len(neuron_spike_bins[start_bin:end_bin]):\n",
    "            neuron_spike_bins[start_bin:end_bin] = binned_spike_times\n",
    "\n",
    "        # Update the start bin for the next image presentation\n",
    "        start_bin = end_bin\n",
    "    return neuron_spike_bins\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    spike_matrix = list(tqdm(executor.map(process_neuron, spike_times.values()), total=num_neurons, desc='Processing neurons'))\n",
    "\n",
    "spike_matrix = torch.stack(spike_matrix)\n",
    "\n",
    "# Convert the spike matrix to a pandas DataFrame and set the index to neuron IDs\n",
    "spike_dataframe = pd.DataFrame(spike_matrix.numpy(), index=spike_times.keys())\n",
    "\n",
    "spike_dataframe.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.6) Create frame column and save spike times as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spike_df = spike_dataframe.T\n",
    "spike_df['frame'] = 'nan'\n",
    "spike_df['frame'] = np.repeat(np.array(stimulus_table['frame']), timesteps_per_frame)\n",
    "\n",
    "#Save the dictionary of valid spike times to a pickle file\n",
    "with open(f'spike_trains_with_stimulus_session_{session_number}.pkl', 'wb') as f:\n",
    "    pickle.dump(spike_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values: 0\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame called 'spike_df'\n",
    "nan_count = spike_df.isna().sum().sum()\n",
    "\n",
    "print(f\"Number of NaN values: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(spike_df.iloc[:,2].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4-KX-9cryhs",
    "tags": []
   },
   "source": [
    "### 1.7) Normalize Firing Rates and save normalized firing rates as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no NaN values in the DataFrame\n"
     ]
    }
   ],
   "source": [
    "### Clean the average firing rates by normalizing the data and removing outliers.\n",
    "# The highest_value removes neurons who have a z score above that value.\n",
    "# The lowest_value removes neurons whose z score never exceeds that value.\n",
    "def clean_avg_firing_rates(df, highest_value, lowest_value):\n",
    "    dfdataframe = df.copy()\n",
    "    dfdataframe = dfdataframe.drop('frame', axis = 1)\n",
    "    \n",
    "    # Normalize the firing rates by calculating z-scores\n",
    "    normalized_firing_rates = (dfdataframe.iloc[:, 1:] - dfdataframe.iloc[:, 1:].mean()) / dfdataframe.iloc[:, 1:].std()\n",
    "\n",
    "    # Add the 'frame' column back to the normalized DataFrame\n",
    "    normalized_firing_rates.insert(0, 'frame', df['frame'])\n",
    "\n",
    "    # Identify the neurons that meet the given criteria\n",
    "    selected_neurons_mask = (~(normalized_firing_rates.iloc[:, 1:] > highest_value).any(axis=0)) & ((normalized_firing_rates.iloc[:, 1:] > lowest_value).any(axis=0))\n",
    "\n",
    "    # Filter the DataFrame based on the selected neurons\n",
    "    filtered_normalized_firing_rates = normalized_firing_rates.loc[:, ['frame'] + selected_neurons_mask[selected_neurons_mask].index.tolist()]\n",
    "    \n",
    "    return filtered_normalized_firing_rates\n",
    "\n",
    "# Use the function with the desired parameters\n",
    "filtered_normalized_firing_rates = clean_avg_firing_rates(spike_df, highest_value=100, lowest_value=0)\n",
    "\n",
    "# Check if there are any NaN values\n",
    "if filtered_normalized_firing_rates.isna().any().any():\n",
    "    print(\"There are NaN values in the DataFrame\")\n",
    "else:\n",
    "    print(\"There are no NaN values in the DataFrame\")\n",
    "    \n",
    "filtered_normalized_firing_rates\n",
    "\n",
    "\n",
    "# Save the dictionary of valid spike times to a pickle file\n",
    "with open(f'filtered_normalized_pickle_{session_number}.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_normalized_firing_rates, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc37wCOHr59L",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a name=\"step2content\"></a>\n",
    "## 2. Visualize the Data\n",
    "[Go to Outline](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_VzlG-Glo97"
   },
   "source": [
    "Here we will look at different aspects of the data we are concerned with. \n",
    "\n",
    "- 2.0) Pull image data from the Allen SDK Package.\n",
    "- 2.1) All natural scenes presented by their frame number. \n",
    "- 2.2) 2.2) Visualize the unique firing pattern of different single units(usually a single neuron).\n",
    "- 2.3) Raster plot of the spike trains for single units over the course of the natural scene testing. \n",
    "- 2.4) Sorted correlation matrix between single units.\n",
    "- 2.5) Grouped Heat Maps of Unit Resposes by Frames\n",
    "- 2.6) T-SNE Plot All Neurons\n",
    "- 2.7) Average firing rates of units during each frame, in Hertz and by Z scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.0) Pull image data from the Allen SDK Package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** To set up the environment. *** \\\n",
    "1st, run 2.0.1. \\\n",
    "2nd, run 2.0.2 if one hasn't already calculated the average firing rate per frame. If they haven't been calculated, run 2.0.3 to load them in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.0.1) Run Cell to set up environment for visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMrnALOflo97",
    "outputId": "3cec57f6-5f7c-45bd-f9b9-ae1e907f005b"
   },
   "outputs": [],
   "source": [
    "# Load packages needed for step 1.\n",
    "!pip install -q allensdk\n",
    "\n",
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
    "\n",
    "# Define Carolina blue color (in RGB format)\n",
    "carolina_blue = (86/255, 160/255, 211/255)\n",
    "\n",
    "# Set output directory to a new folder called 'output' in the current working directory\n",
    "output_dir = os.path.join(os.getcwd(), 'output')\n",
    "\n",
    "# Check if the output directory exists, and create it if it doesn't\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Set DOWNLOAD_COMPLETE_DATASET to True\n",
    "DOWNLOAD_COMPLETE_DATASET = True\n",
    "\n",
    "# Create a file path to the manifest.json file within the output directory\n",
    "manifest_path = os.path.join(output_dir, \"manifest.json\")\n",
    "\n",
    "# Check if the manifest.json file exists\n",
    "if os.path.exists(manifest_path):\n",
    "    print(\"Using existing manifest.json file.\")\n",
    "else:\n",
    "    print(\"Creating a new manifest.json file.\")\n",
    "\n",
    "# Create an instance of the EcephysProjectCache class with the manifest file path as argument\n",
    "cache = EcephysProjectCache(manifest=manifest_path)\n",
    "# Get session table\n",
    "session_table = cache.get_session_table()\n",
    "\n",
    "'''\n",
    " Pick session number\n",
    " '''\n",
    " \n",
    "session_number = 721123822\n",
    "\n",
    "# Pull session.\n",
    "session = cache.get_session_data(session_number,\n",
    "                                 isi_violations_maximum = np.inf,\n",
    "                                 amplitude_cutoff_maximum = np.inf,\n",
    "                                 presence_ratio_minimum = -np.inf\n",
    "                                )\n",
    "# Get spike times.\n",
    "spike_times = session.spike_times\n",
    "# Get specific stimulus table.\n",
    "stimulus_table = session.get_stimulus_table(\"natural_scenes\")\n",
    "\n",
    "# Display objects within session.\n",
    "print(\"Session objects\")\n",
    "print([attr_or_method for attr_or_method in dir(session) if attr_or_method[0] != '_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.0.2) Calulate the firing rates per frame, average firing rate per image, and average firing rate per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average firing rate per frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "timesteps_per_frame = 1  # Set the number of timesteps per frame\n",
    "\n",
    "# Stimulus table.\n",
    "stimulus_table = session.get_stimulus_table(\"natural_scenes\")\n",
    "\n",
    "# The start times of each stimulus presentation\n",
    "image_start_times = torch.tensor(stimulus_table.start_time.values)\n",
    "\n",
    "# The end times of each stimulus presentation\n",
    "image_end_times = torch.tensor(stimulus_table.stop_time.values)\n",
    "\n",
    "# The duration of each image presentation\n",
    "image_durations = image_end_times - image_start_times\n",
    "\n",
    "# The bin size for each image presentation\n",
    "bin_sizes = image_durations / timesteps_per_frame\n",
    "\n",
    "# The number of bins per image presentation\n",
    "bins_per_image = timesteps_per_frame\n",
    "\n",
    "# The total number of bins\n",
    "total_bins = bins_per_image * len(image_start_times)\n",
    "\n",
    "# Create an empty binary spike matrix\n",
    "num_neurons = len(spike_times.keys())\n",
    "\n",
    "def process_neuron(times):\n",
    "    # The start bin for the next image presentation\n",
    "    start_bin = 0\n",
    "    neuron_spike_bins = torch.zeros(total_bins, dtype=torch.int32)\n",
    "    for image_idx, (start_time, end_time) in enumerate(zip(image_start_times, image_end_times)):\n",
    "        # Bin edges for this image presentation\n",
    "        bin_edges = torch.linspace(start_time, end_time, bins_per_image + 1)\n",
    "\n",
    "        # Bin the spike times for this image presentation\n",
    "        binned_spike_times = torch.histc(torch.tensor(times), bins=bin_edges.shape[0]-1, min=bin_edges.min(), max=bin_edges.max())\n",
    "\n",
    "        # Add the binned spike times to the spike matrix\n",
    "        end_bin = start_bin + bins_per_image\n",
    "        if len(binned_spike_times) == len(neuron_spike_bins[start_bin:end_bin]):\n",
    "            neuron_spike_bins[start_bin:end_bin] = binned_spike_times\n",
    "\n",
    "        # Update the start bin for the next image presentation\n",
    "        start_bin = end_bin\n",
    "    return neuron_spike_bins\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    spike_matrix = list(tqdm(executor.map(process_neuron, spike_times.values()), total=num_neurons, desc='Processing neurons'))\n",
    "\n",
    "spike_matrix = torch.stack(spike_matrix)\n",
    "\n",
    "# Convert the spike matrix to a pandas DataFrame and set the index to neuron IDs\n",
    "spike_dataframe = pd.DataFrame(spike_matrix.numpy(), index=spike_times.keys())\n",
    "\n",
    "spikes_per_frame = spike_dataframe.T\n",
    "spikes_per_frame['frame'] = 'nan'\n",
    "spikes_per_frame['frame'] = np.array(stimulus_table['frame'])\n",
    "spikes_per_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate spikes per image by taking the average of the spikes_per_frame calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_per_image = spikes_per_frame.groupby('frame').mean().iloc[1:,1:]\n",
    "spikes_per_image.reset_index(inplace=True)\n",
    "spikes_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the average spikes per second or firing rate in Hertz by multiplying spikes_per_image by 4. Each frame is roughly a quarter second. One can compute the actual average but that is much more computationally intensive(over 7500 times more intesnive). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firing_rate_hz = spikes_per_image * 4\n",
    "firing_rate_hz['frame'] = spikes_per_image['frame']\n",
    "firing_rate_hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary of valid spike times to a pickle file\n",
    "with open(f'spikes_per_frame_{session_number}.pkl', 'wb') as f:\n",
    "    pickle.dump(spikes_per_frame, f)\n",
    "    \n",
    "# Save the dictionary of valid spike times to a pickle file\n",
    "with open(f'spikes_per_image_{session_number}.pkl', 'wb') as f:\n",
    "    pickle.dump(spikes_per_image, f)\n",
    "\n",
    "# Save the dictionary of valid spike times to a pickle file\n",
    "with open(f'firing_rate_hz_{session_number}.pkl', 'wb') as f:\n",
    "    pickle.dump(firing_rate_hz, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.0.3) Load the firing rates per frame, average firing rate per image, and average firing rate per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_per_frame = 'spikes_per_frame_721123822.pkl'\n",
    "spikes_per_image = 'spikes_per_image_721123822.pkl'\n",
    "firing_rate_hz = 'firing_rate_hz_721123822.pkl'\n",
    "\n",
    "# Load the dictionary of valid spike times from the pickle.\n",
    "with open(spikes_per_frame, 'rb') as f:\n",
    "    spikes_per_frame = pickle.load(f)\n",
    "\n",
    "# Load the dictionary of valid spike times from the pickle.\n",
    "with open(spikes_per_image, 'rb') as f:\n",
    "    spikes_per_image = pickle.load(f)\n",
    "\n",
    "# Load the dictionary of valid spike times from the pickle.\n",
    "with open(firing_rate_hz, 'rb') as f:\n",
    "    firing_rate_hz = pickle.load(f)\n",
    "    \n",
    "print(np.shape(spikes_per_frame))\n",
    "print(np.shape(spikes_per_image))\n",
    "print(np.shape(firing_rate_hz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-89IKnQlo97",
    "tags": []
   },
   "source": [
    "### 2.1) All natural scenes presented by their frame number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRVTqemKlo97"
   },
   "source": [
    "In this experiment mice were shown 118 different images of natural scenes around 50 times per image. These images are the events we are trying to predict given the firing rate of each unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Cg2nx0Ilo98",
    "tags": []
   },
   "source": [
    "- Below are all of the images shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJ8pdDF1lo98",
    "outputId": "033deb26-d8e0-42e0-956d-a82bd6f15269"
   },
   "outputs": [],
   "source": [
    "natural_scenes = session.stimulus_presentations[session.stimulus_presentations['stimulus_name']=='natural_scenes']\n",
    "\n",
    "# Pull from where images are located.\n",
    "boc = BrainObservatoryCache(manifest_file='boc/manifest.json')\n",
    "data_set = boc.get_ophys_experiment_data(501498760)\n",
    "\n",
    "# Show all scenes.\n",
    "scene_nums = np.arange(0)\n",
    "\n",
    "# read in the array of images\n",
    "scenes = data_set.get_stimulus_template('natural_scenes')\n",
    "\n",
    "try:\n",
    "    fig, axes = plt.subplots(12, 10, figsize=(20, 20))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(scenes[i], cmap='gray')\n",
    "        ax.set_axis_off()\n",
    "        ax.set_title('scene %d' % i)\n",
    "except IndexError:\n",
    "    pass # ignore the IndexError and continue running the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5O-S6bMlo98"
   },
   "source": [
    "- Below shows when two different images were shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KASbIkt-lo98",
    "outputId": "a19023e2-f4f5-40d6-8be7-6789dcec5b21"
   },
   "outputs": [],
   "source": [
    "### Choose specific scenes to explore.\n",
    "scene_nums = [1,50]\n",
    "\n",
    "### Thank you to http://alleninstitute.github.io/AllenSDK/_static/examples/nb/brain_observatory_stimuli.html for this chunk.\n",
    "boc = BrainObservatoryCache(manifest_file='boc/manifest.json')\n",
    "data_set = boc.get_ophys_experiment_data(501498760)\n",
    "\n",
    "# read in the array of images\n",
    "scenes = data_set.get_stimulus_template('natural_scenes')\n",
    "\n",
    "# Define a function that takes two arguments: a dataframe containing information about stimulus trials and a title for the plot.\n",
    "# The function plots the times each specified stimulus occurs throught the timeframe.\n",
    "def plot_stimulus_table(natural_scenes, title):\n",
    "    fstart = natural_scenes.start_time.min()\n",
    "    fend = natural_scenes.stop_time.max()\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,1))\n",
    "    ax = fig.gca()\n",
    "    # Loop over each trial in the dataframe\n",
    "    for i, trial in natural_scenes.iterrows():    \n",
    "        # Calculate the start and stop times for the trial.\n",
    "        x1 = float(trial.start_time - fstart) / (fend - fstart)\n",
    "        x2 = float(trial.stop_time - fstart) / (fend - fstart)      \n",
    "        # Add a rectangle to the plot.\n",
    "        ax.add_patch(patches.Rectangle((x1, 0.0), x2 - x1, 1.0, color='r'))\n",
    "        \n",
    "    ax.set_xticks((0,1))\n",
    "    ax.set_xticklabels((int(np.round(fstart)), int(np.round(fend))))\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(\"Timestamps where the Specific Scenes Appears \\n During the Natural Scenes\")\n",
    "    ax.set_xlabel(title)\n",
    "\n",
    "# read in the array of images\n",
    "scenes = data_set.get_stimulus_template('natural_scenes')\n",
    "\n",
    "# display a couple of the scenes\n",
    "fig, axes = plt.subplots(1,len(scene_nums))\n",
    "for ax,scene in zip(axes, scene_nums):\n",
    "    ax.imshow(scenes[scene,:,:], cmap='gray')\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title('scene %d' % scene)\n",
    "    \n",
    "# build up a mask of trials for which one of a list of scenes is visible\n",
    "trial_mask = natural_scenes.frame == -2\n",
    "for scene in scene_nums:\n",
    "    trial_mask |= (natural_scenes.frame == scene)\n",
    "natural_scenes = natural_scenes[trial_mask]\n",
    "\n",
    "# plot the trials\n",
    "plot_stimulus_table(natural_scenes, \"scenes %s \" % scene_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2Pvv_54lo98",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.2) Visualize the unique firing pattern of different single units(usually a single neuron)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZK3_LTtlo98"
   },
   "source": [
    "This unique firing pattern allows one to differenciate between two possible neurons(units) and create a unique spike train for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyo2wi-Plo98",
    "outputId": "19d10f1d-9a25-45a8-b9ec-28be7f25bc7a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the mean spike waveforms for each neuron\n",
    "mean_waveforms = session.mean_waveforms\n",
    "\n",
    "# Number of waveforms to display (you can change this value)\n",
    "n_waveforms = 10\n",
    "unit_ids = session.units.index.values\n",
    "\n",
    "# Plot the mean spike waveforms for the first n_waveforms neurons\n",
    "fig, axes = plt.subplots(n_waveforms, 1, figsize=(8, 2 * n_waveforms), sharex=True)\n",
    "for i, (unit_id, ax) in enumerate(zip(unit_ids[:n_waveforms], axes)):\n",
    "    waveform = mean_waveforms[unit_id]\n",
    "    ax.plot(waveform.T)\n",
    "    ax.set_title(f'Neuron {unit_id} Mean Waveform')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.xlabel('Time (samples)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGyOdyialo98",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.3) Raster plot of the spike trains for single units over the course of the natural scene testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzpcDgd_lo98"
   },
   "source": [
    "When the spike trains are created for each unique unit, they produce a 1 for every spike and 0 if they are not spiking at each timestep. The spike trains are plotted below for a few unique units over the natural scene time frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0wJm6v6lo99"
   },
   "source": [
    "*** Add Raster Plot for binned data. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcyhU3vElo99",
    "outputId": "06b54d5c-d0dc-4b83-b666-2d93bc7cbc9b"
   },
   "outputs": [],
   "source": [
    "natural_scenes = session.stimulus_presentations[session.stimulus_presentations['stimulus_name'] == 'natural_scenes']\n",
    "available_unit_ids = session.spike_times.keys()\n",
    "all_spike_times = {unit_id: session.spike_times[unit_id] for unit_id in available_unit_ids}\n",
    "units = cache.get_units()\n",
    "\n",
    "# Create a function to build a raster plot.\n",
    "def visualize_spike_data(num_units_to_visualize, all_spike_times, units, start_time=0, end_time=10, num_timesteps=30):\n",
    "    # Select a subset of units to plot\n",
    "    available_unit_ids = list(all_spike_times.keys())\n",
    "    subset_unit_ids = available_unit_ids[:num_units_to_visualize]\n",
    "\n",
    "    # Calculate the time_interval (in seconds) from the first unit's sampling rate\n",
    "    time_interval = 1 / units.iloc[0]['sampling_rate']\n",
    "\n",
    "    # Create the raster plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    for i, unit_id in enumerate(subset_unit_ids):\n",
    "        spike_times = all_spike_times[unit_id]\n",
    "\n",
    "        # Filter the spike times based on start_time and end_time\n",
    "        filtered_spike_times = [t for t in spike_times if start_time <= t <= end_time]\n",
    "\n",
    "        ax.scatter(filtered_spike_times, [i] * len(filtered_spike_times), marker='|')\n",
    "\n",
    "    ax.set_xlim([start_time, end_time])\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Unit')\n",
    "    ax.set_yticks(range(len(subset_unit_ids)))\n",
    "    ax.set_yticklabels(subset_unit_ids)\n",
    "    ax.set_title('Raster plot for a subset of units')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Create an array of time bins\n",
    "    time_bins = np.arange(start_time, start_time + num_timesteps, time_interval)\n",
    "\n",
    "    # Only consider the first 'num_timesteps' time bins\n",
    "    time_bins = time_bins[:num_timesteps]\n",
    "\n",
    "    # Initialize an empty DataFrame with the time bins as the index\n",
    "    spike_counts_df = pd.DataFrame(index=time_bins[:-1])\n",
    "\n",
    "    # Iterate through the units and count the spikes in each time bin\n",
    "    for unit_id in subset_unit_ids:\n",
    "        spike_times = all_spike_times[unit_id]\n",
    "        spike_counts, _ = np.histogram(spike_times, bins=time_bins)\n",
    "        spike_counts_df[unit_id] = spike_counts\n",
    "\n",
    "    # Rename the index\n",
    "    spike_counts_df.index.name = 'Time (s)'\n",
    "\n",
    "    return spike_counts_df\n",
    "\n",
    "# Example usage:\n",
    "num_units_to_visualize = 10\n",
    "start_time = natural_scenes.start_time.min()\n",
    "end_time = natural_scenes.stop_time.max()\n",
    "num_timesteps = 1000\n",
    "\n",
    "spike_counts_df = visualize_spike_data(num_units_to_visualize, all_spike_times, units, start_time, end_time, num_timesteps)\n",
    "print('The following is the first 1000 timesteps of the data visualized. 1/30,000th of a second.')\n",
    "print(np.unique(spike_counts_df))\n",
    "spike_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugTRoB9Ylo99",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.4) Sorted correlation matrix between single units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sorted correlation matrix between units based on their firing rates. A clustering method called Agglomerative hierarchical cluster was group the correlation matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8zXS-Z5lo99",
    "outputId": "775b1398-4c8a-437b-c691-c90f131a4f8e"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "X = filtered_normalized_firing_rates.drop(columns=['frame'])\n",
    "\n",
    "### This function calculates the correlation coefficient between each pair of neurons in the input matrix X (in our case, firing rates).\n",
    "def custom_corrcoef(X):\n",
    "    # Calculate the correlation coefficient matrix by taking the dot product of the normalized matrix and its transpose\n",
    "    # Divide the result by the number of columns in X to normalize the sum\n",
    "    return np.dot(X, X.T) / X.shape[1]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = custom_corrcoef(X.T)\n",
    "\n",
    "# Compute the hierarchical clustering\n",
    "distance_matrix = 1 - np.abs(corr_matrix)\n",
    "np.fill_diagonal(distance_matrix, 0)  # Set the diagonal to zero\n",
    "linked = linkage(squareform(distance_matrix), method='average')  # The linkage function performs the cluster.\n",
    "\n",
    "# Plot the heatmap with dendrogram-based sorting\n",
    "sns.clustermap(corr_matrix, cmap='inferno', row_linkage=linked, col_linkage=linked, vmin=0, vmax=1)\n",
    "plt.title(\"Sorted correlation matrix of firing rates\")\n",
    "plt.xlabel(\"Neuron index\")\n",
    "plt.ylabel(\"Neuron index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0uLbJL-lo99"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "X = filtered_normalized_firing_rates.drop(columns=['frame'])\n",
    "\n",
    "### This function calculates the correlation coefficient between each pair of neurons in the input matrix X (in our case, firing rates).\n",
    "def custom_corrcoef(X):\n",
    "    # Calculate the correlation coefficient matrix by taking the dot product of the normalized matrix and its transpose\n",
    "    # Divide the result by the number of columns in X to normalize the sum\n",
    "    return np.dot(X, X.T) / X.shape[1]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = custom_corrcoef(X.T)\n",
    "\n",
    "# Compute the hierarchical clustering\n",
    "distance_matrix = 1 - np.abs(corr_matrix)\n",
    "np.fill_diagonal(distance_matrix, 0)  # Set the diagonal to zero\n",
    "linked = linkage(squareform(distance_matrix), method='average') \n",
    "\n",
    "# Plot the heatmap with dendrogram-based sorting\n",
    "sns.clustermap(corr_matrix, cmap='inferno', row_linkage=linked, col_linkage=linked, vmin=-1, vmax=1)\n",
    "plt.title(\"Sorted correlation matrix of firing rates\")\n",
    "plt.xlabel(\"Neuron index\")\n",
    "plt.ylabel(\"Neuron index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.x) Show Correlation matrices between time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxzL9pvtlo99",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2. 5) Grouped heat maps of unit responses by frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filtered_normalized_firing_rates(df, num_frames):\n",
    "    # Calculate the correlation between the firing rates and frames\n",
    "    correlation_matrix = df.corrwith(df['frame'], method='spearman')\n",
    "\n",
    "    # Sort neurons by their correlation with the 'frame' column\n",
    "    sorted_neurons = correlation_matrix[df.columns[1:]].sort_values().index.tolist()\n",
    "\n",
    "    # Filter the DataFrame based on the desired number of frames\n",
    "    filtered_df = df.head(num_frames)\n",
    "\n",
    "    # Sort neurons by their correlation with the 'frame' column\n",
    "    sorted_filtered_df = filtered_df[['frame'] + sorted_neurons]\n",
    "\n",
    "    # Sort the DataFrame by the 'frame' column\n",
    "    sorted_filtered_df.sort_values(by='frame', inplace=True)\n",
    "\n",
    "    # Set the 'frame' column as the index of the DataFrame\n",
    "    sorted_filtered_df.set_index('frame', inplace=True)\n",
    "\n",
    "    # Create a heatmap for the filtered and sorted DataFrame\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    ax = sns.heatmap(sorted_filtered_df.T, cmap='viridis', yticklabels=False)\n",
    "\n",
    "    # Get unique frame values\n",
    "    unique_frames = sorted_filtered_df.index.unique()\n",
    "\n",
    "    # Set x-ticks at the middle of each frame group\n",
    "    ax.set_xticks([np.where(sorted_filtered_df.index == frame)[0].mean() for frame in unique_frames])\n",
    "\n",
    "    # Set x-tick labels to be the frame numbers\n",
    "    ax.set_xticklabels(unique_frames)\n",
    "\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Neuron')\n",
    "    plt.title(f'Filtered and Sorted Normalized Firing Rates')\n",
    "    plt.show()\n",
    "\n",
    "# Use the function with the desired parameters\n",
    "plot_filtered_normalized_firing_rates(firing_rates_per_frame, num_frames=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function with the desired parameters\n",
    "plot_filtered_normalized_firing_rates(average_firing_rate, num_frames=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only frames 0, 1, 2, 3, 4, 5\n",
    "frames_to_select = [1,2,3,4,5]\n",
    "select_images = average_firing_rate[average_firing_rate['frame'].isin(frames_to_select)]\n",
    "select_images\n",
    "\n",
    "plot_filtered_normalized_firing_rates(select_images, num_frames=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.6) T-SNE plot all neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pre-process for t-SNE.\n",
    "\n",
    "# Drop the 'frame' column and convert the DataFrame to a NumPy array\n",
    "tsne_df = filtered_normalized_firing_rates.drop(columns=['frame'])\n",
    "tsne_array = np.array(tsne_df).transpose()\n",
    "\n",
    "# Perform 2D t-SNE.\n",
    "tsne_2d = TSNE(n_components=2, random_state=69)\n",
    "tsne_2d_results = tsne_2d.fit_transform(tsne_array)\n",
    "\n",
    "# Plot 2D t-SNE with Carolina blue color\n",
    "plt.scatter(tsne_2d_results[:, 0], tsne_2d_results[:, 1], color=carolina_blue, marker='.')\n",
    "plt.title('2D t-SNE of Neurons Shown 117 different Images')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only frames 0, 1, 2, 3, 4, 5\n",
    "frames_to_select = [1,9]\n",
    "select_images = filtered_normalized_firing_rates[filtered_normalized_firing_rates['frame'].isin(frames_to_select)]\n",
    "\n",
    "\n",
    "# Drop the 'frame' column and convert the DataFrame to a NumPy array\n",
    "tsne_df = select_images.drop(columns=['frame'])\n",
    "\n",
    "tsne_array = np.array(tsne_df).transpose()\n",
    "# Perform 2D t-SNE.\n",
    "tsne_2d = TSNE(n_components=2, random_state=42)\n",
    "tsne_2d_results = tsne_2d.fit_transform(tsne_array)\n",
    "\n",
    "# Plot 2D t-SNE with Carolina blue color\n",
    "plt.scatter(tsne_2d_results[:, 0], tsne_2d_results[:, 1], color=carolina_blue, marker='.')\n",
    "plt.title('2D t-SNE of Neurons Shown Two Images')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# select only frames 0, 1, 2, 3, 4, 5\n",
    "frames_to_select = [1,9]\n",
    "select_images = filtered_normalized_firing_rates[filtered_normalized_firing_rates['frame'].isin(frames_to_select)]\n",
    "\n",
    "# Drop the 'frame' column and convert the DataFrame to a NumPy array\n",
    "tsne_df = select_images.drop(columns=['frame'])\n",
    "\n",
    "tsne_array = np.array(tsne_df).transpose()\n",
    "# Perform 2D t-SNE.\n",
    "tsne_2d = TSNE(n_components=2, random_state=42)\n",
    "tsne_2d_results = tsne_2d.fit_transform(tsne_array)\n",
    "\n",
    "# Define the number of clusters\n",
    "num_clusters = 2  # Change this to the number of clusters you want\n",
    "\n",
    "# Perform KMeans clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(tsne_2d_results)\n",
    "\n",
    "# Get the cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Define the colors for each cluster\n",
    "colors = ['skyblue', 'red']  # Change these to the colors you want\n",
    "\n",
    "# Set the background color to black\n",
    "plt.rcParams['axes.facecolor'] = 'black'\n",
    "\n",
    "# Plot 2D t-SNE with colors representing different clusters\n",
    "for i in range(num_clusters):\n",
    "    plt.scatter(tsne_2d_results[labels == i, 0], tsne_2d_results[labels == i, 1], color=colors[i])\n",
    "\n",
    "plt.title('2D t-SNE of Neurons Shown Two Images')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "# plt.legend()  # Commented out to remove the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to plot the unit twice, once for each frame, then color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.7) Average firing rates of units during each frame, in Hertz and by Z scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose how many neurons to display.\n",
    "num_of_neurons_display = 12\n",
    "\n",
    "# Set the background color to black\n",
    "plt.rcParams['axes.facecolor'] = 'black'\n",
    "\n",
    "# Plot the firing rate for each neuron\n",
    "firing_rate_hz.iloc[:,1:(num_of_neurons_display+1)].plot(kind='line', legend=False)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Mean Firing Rate in Hertz')\n",
    "plt.title('Mean Firing Rate in Hertz per Frame')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the background color to black\n",
    "plt.rcParams['axes.facecolor'] = 'black'\n",
    "\n",
    "# Plot the firing rate for each neuron\n",
    "firing_rate_hz.iloc[:,1:].plot(kind='line', legend=False)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Mean Firing Rate in Hertz')\n",
    "plt.title('Mean Firing Rate in Hertz per Frame')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_spikes_per_frame = filtered_normalized_firing_rates.groupby('frame').mean().iloc[1:,1:]\n",
    "normalized_spikes_per_frame.reset_index(inplace=True)\n",
    "\n",
    "# Set the background color to black\n",
    "plt.rcParams['axes.facecolor'] = 'black'\n",
    "\n",
    "# Plot the firing rate for each neuron\n",
    "normalized_spikes_per_frame.iloc[:,1:].plot(kind='line', legend=False)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Mean Firing Rate in Hertz')\n",
    "plt.title('Normalized Firing Rate per Frame')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fEJNJE7Vxu0",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a name=\"step3content\"></a>\n",
    "## 3. Image Prediction Modeling\n",
    "[Go to Outline](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QPrZ8hZt9kb"
   },
   "source": [
    "    The goal of the image prediction modeling is to predict what image is shown based on the neuropixel data given.\n",
    "\n",
    "- 3.0) Create train splits, test splits, and a correlation based adjacency matrix.\n",
    "- 3.1) Baseline Model: a random guess with an accuracy of 0.85% (1/118).\n",
    "- 3.2) Multiclass Regression\n",
    "- 3.3) Support Vector Machine with Radial Basis Function.\n",
    "- 3.4) Principal Component Regression.\n",
    "- 3.5) Neural Network with one hidden layer.\n",
    "- 3.6) Deep Neural Network\n",
    "- 3.7) Long-Short Term Memory model.\n",
    "- 3.8) Static Graph Neural Network.\n",
    "- 3.9) Graph Attention Network\n",
    "- 3.10) static ST-GNN.\n",
    "- 3.11) Dynamic STGNN with adjacency matrix for each frame.\n",
    "- 3.12) Dynamic STGNN with adjacency matrix for each timestep and frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5vbGbzRuiWk",
    "tags": []
   },
   "source": [
    "### 3.0) Create train splits, test splits, and a correlation based adjacency matrix.\n",
    "\n",
    "- Note: The 4d train/test splits will overwrite the 2d splits. I need to change the models that use the 2d dataframes to pull the information from the 4d. This will speed up loading time and cut down on data being used by memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create 4D train/test splits as well as matrices. Using normalized spike trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val shape: torch.Size([472, 10, 1141, 1])\n",
      "y_val shape: torch.Size([472, 1])\n",
      " \n",
      "X (array): Matrix of number of batch_size, time_steps_per_frame, num_nodes, and number of features per node.\n",
      "X.shape = (B, T, N, F)\n",
      "X_train shape: torch.Size([4248, 10, 1141, 1])\n",
      "X_test shape: torch.Size([1180, 10, 1141, 1])\n",
      "X_train type: <class 'torch.Tensor'>\n",
      " \n",
      "X_val shape: torch.Size([472, 10, 1141, 1])\n",
      "y_val shape: torch.Size([472, 1])\n",
      " \n",
      "y_shape = [batch_size, unique_frames_shown_per_10_timesteps]\n",
      "y_train shape: torch.Size([4248, 1])\n",
      "y_test shape: torch.Size([1180, 1])\n",
      "y_test type: <class 'torch.Tensor'>\n",
      " \n",
      "Shape of edge_index torch.Size([1141, 1141])\n",
      "Edge Index tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "### 3.10) STGNN Implementation.\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set sequence length and determine new number of samples\n",
    "seq_len = 10 # Set sequence length\n",
    "batch_size = 32\n",
    "\n",
    "filtered_normalized_firing_rates = filtered_normalized_firing_rates[filtered_normalized_firing_rates['frame'] != -1]\n",
    "# Create X and y.\n",
    "X = filtered_normalized_firing_rates.drop(columns=['frame']).values\n",
    "y = filtered_normalized_firing_rates['frame'].values\n",
    "\n",
    "# Encode categorical target values\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Create train/test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Split the training set further into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, shuffle=False)\n",
    "\n",
    "num_samples_train = X_train.shape[0] // seq_len\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y)) - 1\n",
    "\n",
    "# Determine the new number of samples.\n",
    "num_samples_test = X_test.shape[0] // seq_len\n",
    "\n",
    "# Reshape input to have 3 dimensions. (samples, timesteps, features/units)\n",
    "X_train = X_train[:num_samples_train*seq_len]  \n",
    "X_train = X_train.reshape(num_samples_train, seq_len, num_features)\n",
    "\n",
    "# Reshape output to match input\n",
    "y_train = y_train[:num_samples_train*seq_len]  # Make the total length divisible by seq_len\n",
    "y_train = y_train.reshape(num_samples_train, seq_len, 1)  # Keep as 2D tensor\n",
    "y_train = y_train[:, -1]  # Predicting a single value for each sequence\n",
    "\n",
    "# Reshape input to be 3D (samples, timesteps, features)\n",
    "X_test = X_test[:num_samples_test*seq_len]  # Make the total length divisible by seq_len\n",
    "X_test = X_test.reshape(num_samples_test, seq_len, num_features)\n",
    "\n",
    "# Reshape output to match input\n",
    "y_test = y_test[:num_samples_test*seq_len]  # Make the total length divisible by seq_len\n",
    "y_test = y_test.reshape(num_samples_test, seq_len, 1)  # Keep as 2D tensor\n",
    "y_test = y_test[:, -1]  # Predicting a single value for each sequence\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_LSTM = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_test_LSTM = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Add an extra dimension for features\n",
    "X_train = X_train_LSTM.unsqueeze(2)\n",
    "X_test = X_test_LSTM.unsqueeze(2)\n",
    "\n",
    "\n",
    "# Permute the dimensions to match the expected input shape\n",
    "X_train = X_train.permute(0, 1, 3, 2)\n",
    "X_test = X_test.permute(0, 1, 3, 2)\n",
    "\n",
    "# Fold 4D array into 3D. Necessary for LSTM.\n",
    "def reshape_data(data):\n",
    "    B, T, N, F = data.size()\n",
    "    return data.view(B, T, N * F)\n",
    "    \n",
    "X_train_reshaped = reshape_data(X_train).to(device)\n",
    "X_test_reshaped = reshape_data(X_test).to(device)\n",
    "\n",
    "# Validation setup\n",
    "num_samples_val = X_val.shape[0] // seq_len\n",
    "X_val = X_val[:num_samples_val*seq_len]\n",
    "X_val = X_val.reshape(num_samples_val, seq_len, num_features)\n",
    "y_val = y_val[:num_samples_val*seq_len]\n",
    "y_val = y_val.reshape(num_samples_val, seq_len, 1)\n",
    "y_val = y_val[:, -1]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_val_LSTM = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Add an extra dimension for features and permute for validation set\n",
    "X_val = X_val_LSTM.unsqueeze(2)\n",
    "X_val = X_val.permute(0, 1, 3, 2)\n",
    "X_val_reshaped = reshape_data(X_val).to(device)\n",
    "\n",
    "train_loader = DataLoader(list(zip(X_train_reshaped, y_train)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(X_test_reshaped, y_test)), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(list(zip(X_val_reshaped, y_val)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(\" \")\n",
    "print(f\"X (array): Matrix of number of batch_size, time_steps_per_frame, num_nodes, and number of features per node.\")\n",
    "print(\"X.shape = (B, T, N, F)\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"X_train type: {type(X_train)}\")\n",
    "print(\" \")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(\" \")\n",
    "print(f\"y_shape = [batch_size, unique_frames_shown_per_{seq_len}_timesteps]\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"y_test type: {type(y_test)}\")\n",
    "print(\" \")\n",
    "# Define a correlation-based edge index\n",
    "corr_matrix = np.corrcoef(X, rowvar=False)\n",
    "\n",
    "# Convert the correlation matrix to an edge index tensor\n",
    "edge_index = corr_matrix  #find the indices where the correlation matrix is not 0\n",
    "edge_index = torch.tensor(edge_index.T, dtype=torch.long)  # convert to a PyTorch tensor and transpose\n",
    "print('Shape of edge_index', np.shape(edge_index))\n",
    "print('Edge Index',edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mav5i5bbt938",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.1) Baseline Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The baseline model assumes a random guess, with an accuracy of 0.85% (1/117)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1684255168483,
     "user": {
      "displayName": "Raywzhere",
      "userId": "17705554742553511920"
     },
     "user_tz": 240
    },
    "id": "DV6hmsCC7QbJ",
    "outputId": "573793ec-caf5-468c-fd19-48291b1a47b5"
   },
   "outputs": [],
   "source": [
    "print(f'Test Accuracy: {np.round(1/117*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.2) Multiclass Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBInmft27Rzv",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.3) SVM with a Radial Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143339,
     "status": "ok",
     "timestamp": 1684256160638,
     "user": {
      "displayName": "Raywzhere",
      "userId": "17705554742553511920"
     },
     "user_tz": 240
    },
    "id": "MblZGbHv7SQo",
    "outputId": "5fa7e463-2652-4bfa-c975-61e48e7fe0bc"
   },
   "outputs": [],
   "source": [
    "# Create and train the SVM model with a radial kernel\n",
    "model = SVR(kernel='rbf', C=1.0, gamma='scale')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set and round them to the nearest integer\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_rounded = [round(pred) for pred in y_pred]\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "correct_predictions = np.sum(np.equal(y_test, y_pred_rounded))\n",
    "accuracy = correct_predictions / len(y_test) * 100\n",
    "print(f\"SVM Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define the RBF kernel\n",
    "def rbf_kernel(x1, x2, sigma=1.0):\n",
    "    x1 = x1.unsqueeze(1)  # Shape: [N1, 1, D]\n",
    "    x2 = x2.unsqueeze(0)  # Shape: [1, N2, D]\n",
    "    squared_distance = ((x1 - x2) ** 2).sum(2)  # Shape: [N1, N2]\n",
    "    return torch.exp(-squared_distance / (2 * sigma ** 2))  # Shape: [N1, N2]\n",
    "\n",
    "class SVM(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super(SVM, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.alphas = torch.nn.Parameter(torch.randn(n_features, n_classes))  # Change this line\n",
    "        \n",
    "    def forward(self, x):\n",
    "        kernel_matrix = rbf_kernel(x, x)\n",
    "        output = kernel_matrix @ self.alphas\n",
    "        return output\n",
    "\n",
    "# Define the loss function (hinge loss)\n",
    "def hinge_loss(outputs, labels):\n",
    "    labels = torch.eye(svm.n_classes)[labels]  # One-hot encoding\n",
    "    return torch.mean(torch.clamp(1 - outputs.t() * labels, min=0))  # Element-wise hinge loss\n",
    "\n",
    "# Instantiate the SVM\n",
    "svm = SVM(n_features=X_train.shape[1], n_classes=len(torch.unique(y_train)))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(svm.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = svm(X_train)\n",
    "    loss = hinge_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the SVM model with a radial kernel\n",
    "model = SVR(kernel='rbf', C=.5, gamma='scale')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set and round them to the nearest integer\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_rounded = [round(pred) for pred in y_pred]\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "correct_predictions = sum(y_test == y_pred_rounded)\n",
    "accuracy = correct_predictions / len(y_test) * 100\n",
    "print(f\"SVM Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.4) Run Principal Component Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2604,
     "status": "ok",
     "timestamp": 1684255182894,
     "user": {
      "displayName": "Raywzhere",
      "userId": "17705554742553511920"
     },
     "user_tz": 240
    },
    "id": "wUYYEu_v7TK8",
    "outputId": "8dfaaa53-2599-4c99-804a-8606633ab9a0"
   },
   "outputs": [],
   "source": [
    "### A basic PCR.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a pipeline for PCR\n",
    "n_components = 10  # Adjust the number of components based on your data\n",
    "pcr = Pipeline([\n",
    "    ('pca', PCA(n_components=n_components)),\n",
    "    ('linear_regression', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train the PCR model\n",
    "pcr.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred = pcr.predict(X_test)\n",
    "\n",
    "# Convert predictions and true labels to integers (since you mentioned that each number represents a picture)\n",
    "y_pred_int = np.round(y_pred).astype(int)\n",
    "y_test_int = y_test.astype(int)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test_int, y_pred_int)\n",
    "print(f\"PCR accuracy: {np.round(accuracy*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8x3G8Eplo9-",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.5) Neural Network with one hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supports the use of Deep neural netowrks over linear models. http://cs230.stanford.edu/projects_winter_2021/reports/70532925.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def torch_nn(num_of_neurons_in_dense, batch_size, epochs):\n",
    "    # Define the model\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, input_dim, num_of_neurons_in_dense, num_classes):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, num_of_neurons_in_dense)\n",
    "            self.fc2 = nn.Linear(num_of_neurons_in_dense, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.sigmoid(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = Net(X_train.shape[1], num_of_neurons_in_dense, num_classes)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return print(f'Model Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "num_of_neurons_in_dense = 1000\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "\n",
    "torch_nn(num_of_neurons_in_dense, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def torch_nn_grid_search(neuron_range, batch_size, epochs):\n",
    "    accuracies = []\n",
    "\n",
    "    for num_of_neurons_in_dense in neuron_range:\n",
    "        # Define the model\n",
    "        class Net(nn.Module):\n",
    "            def __init__(self, input_dim, num_of_neurons_in_dense, num_classes):\n",
    "                super(Net, self).__init__()\n",
    "                self.fc1 = nn.Linear(input_dim, num_of_neurons_in_dense)\n",
    "                self.fc2 = nn.Linear(num_of_neurons_in_dense, num_classes)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = torch.sigmoid(self.fc1(x))\n",
    "                x = self.fc2(x)\n",
    "                return x\n",
    "\n",
    "        # Instantiate the model\n",
    "        model = Net(X_train.shape[1], num_of_neurons_in_dense, num_classes)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        accuracies.append(accuracy)\n",
    "        print(f'Neurons: {num_of_neurons_in_dense}, Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    # Plot the accuracies\n",
    "    plt.plot(neuron_range, accuracies, marker='o')\n",
    "    plt.title('Model Accuracy vs. Number of Neurons in Dense Layer')\n",
    "    plt.xlabel('Number of Neurons')\n",
    "    plt.ylabel('Model Test Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "neuron_range = range(50, 2000, 50)\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "\n",
    "torch_nn_grid_search(neuron_range, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def torch_nn(num_of_neurons_in_dense, batch_size, epochs):\n",
    "    # Define the model\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, input_dim, num_of_neurons_in_dense, num_classes):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, num_of_neurons_in_dense)\n",
    "            self.fc2 = nn.Linear(num_of_neurons_in_dense, num_of_neurons_in_dense // 2)\n",
    "            self.fc3 = nn.Linear(num_of_neurons_in_dense // 2, num_of_neurons_in_dense // 4)\n",
    "            self.fc4 = nn.Linear(num_of_neurons_in_dense // 4, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.sigmoid(self.fc1(x))\n",
    "            x = torch.sigmoid(self.fc2(x))\n",
    "            x = torch.sigmoid(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = Net(X_train.shape[1], num_of_neurons_in_dense, num_classes)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return print(f'Model with multiple layers Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "num_of_neurons_in_dense = 1000\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "\n",
    "torch_nn(num_of_neurons_in_dense, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.6) Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK16DzNQnOY7",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.7) Long-Short Term Memory model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "   The purpose of the following Long Short-Term Memory (LSTM) model is to account for temporal dependencies while using deep learning to predict a categorical outcome. Given a sequence of inputs $x_1, . . . , x_T$, the LSTM model attempts to predict a corresponding sequence of outputs $y_1, . . . , y_{T'}$. The sequence of inputs are the firing rates of indivual units and the sequence of outputs is the log probability for each frame. It does this by two states, one for long term time dependencies and one for short term. These are referred to as the cell and hidden states. In our model, we make a prediction from the frame that has the highest log probability.\n",
    "\n",
    "   Formally, this can be described as the LSTM model learning a probability distribution over sequences of outputs conditioned on sequences of inputs, expressed as:\n",
    "\n",
    "$$\n",
    "p(y_1, . . . , y_{T'} | x_1, . . . , x_T ) = \\prod_{t=1}^{T'} p(y_t|h_{t-1}, c_{t-1}, x_t)\n",
    "$$\n",
    "\n",
    "   Where $x_t$ is the firing rate at each timestep $t$, $x_T$ is all the firing rates for each uniqe frame, $y_t$ is the log probability of the frame shown at each timestep $t$, $y_T$ is the log probability of which frame was shown for each frame. $h_{t-1}$ is the hidden state at time $t-1$, and $c_{t-1}$ is the cell state at time $t-1$.\n",
    "\n",
    "   The ability to selectively forget information comes from its gating mechanisms. This is useful for when time dependencies might change. Each frame is shown for a quarter of a second, or 0.25 seconds. Say our data has been processed where each unique frame shown is split between 10 rows/timesteps. When we are making a prediction about each frame the model uses the output of the model from the final timestep of that frame. In this case we would only make one guess per unique frame shown. \n",
    "\n",
    "   This LSTM cell takes as input the current firing rate of each neuron $x_t$ and the previous hidden state $h_{t-1}$, and outputs a new hidden state $h_t$ and a new cell state $c_t$. The hidden state represents the short term memory while the cell state represents the long term memory. This process can be described by the following equations:\n",
    "\n",
    "1. **Forget gate:**\n",
    "    $$\n",
    "    f_t = \\sigma(W_{if} \\cdot x_t + b_{if} + W_{hf} \\cdot h_{t-1} + b_{hf})\n",
    "    $$\n",
    "\n",
    "2. **Input gate:**\n",
    "    $$\n",
    "    i_t = \\sigma(W_{ii} \\cdot x_t + b_{ii} + W_{hi} \\cdot h_{t-1} + b_{hi})\n",
    "    $$\n",
    "\n",
    "3. **Candidate cell state:**\n",
    "    $$\n",
    "    g_t = \\tanh(W_{ig} \\cdot x_t + b_{ig} + W_{hg} \\cdot h_{t-1} + b_{hg})\n",
    "    $$\n",
    "\n",
    "4. **Update cell state:**\n",
    "   $$\n",
    "   c_t = f_t \\odot c_{t-1} + i_t \\odot g_t\n",
    "   $$\n",
    "   \n",
    "5. **Output gate:**\n",
    "   $$\n",
    "   o_t = \\sigma(W_{io} \\cdot x_t + b_{io} + W_{ho} \\cdot h_{t-1} + b_{ho})\n",
    "   $$\n",
    "   \n",
    "6. **Update hidden state:**\n",
    "   $$\n",
    "   h_t = o_t \\odot \\tanh(c_t)\n",
    "   $$\n",
    "  \n",
    "7. **Output layer:**\n",
    "   $$\n",
    "   y_t = W_y \\cdot h_t + b_y\n",
    "   $$\n",
    "  \n",
    "8. **Prediction:**\n",
    "   $$\n",
    "   \\hat{y}_t = argmax ( softmax (y_T))\n",
    "   $$\n",
    "\n",
    "      The model uses the forget gate(step 1), input gate(step 2), and the canidate cell state(step 3) to update the long term memory(step 4). It then creates the output gate(step 5), which represents the short term memory. This step only accounts for information from the current unique frame. The model then takes into account both long and short term memory by using the Hadamard product between the output gate and the $tanh$ of the cell state.\n",
    "      The output ${y}_t$ is a vector of size equal to the number of classes in the output space. $\\hat{y}_t$ Each element of this vector represents the score for a particular class. The class with the highest score is the model's prediction.\n",
    "\n",
    "#### LSTM Parameters\n",
    "\n",
    "- $x_t$: the input vector at time $t$. This is the firing rates of all neurons at time $t$. It is a vector where $m$ represents the number of features. In our case it is equal to the number of neurons in our dataset.\n",
    "    $$\n",
    "    x_{t} = x_{1,t} ,..., x_{m,t}\n",
    "    $$    \n",
    "\n",
    "- $h_{t-1}$: the hidden state at time $t-1$. This is the output of the LSTM cell from the previous time step. It also represents the short term memory. It is a vector. Where \"n\" represents the hyperparameter size known as the hidden dimension. It is set by \"hidden_dim\" in this program. In this model batch processing is used and the short-term memory resets between frames. This means the model is considered \"stateless\"(opposed to \"stateful\"). This means that the hidden state information doesn't carry over between each frame. The pictures shown aren't related, so the direct temporal information from the timestep from one unique frame to the other may not matter. This is why we uses \"stateless\" here.\n",
    "    $$\n",
    "    h_{t-1} = h_{1,t-1},...,h_{n,t-1}\n",
    "    $$    \n",
    "\n",
    "- $c_{t-1}$: the cell state at time $t-1$, i.e., the internal memory of the LSTM cell from the previous time step. Similar to the past long term memory.\n",
    " \n",
    "- $W_{if}, W_{ii}, W_{ig}, W_{io}$: weight matrices for the forget gate, input gate, cell candidate, and the output gate for $x_t$. If the number of features of $x_t$ are $m$ and the hidden dimensions are $n$, then these weight matrices are shape $n \\times m$.\n",
    "\n",
    "- $W_{hf}, W_{hi}, W_{hg}, W_{ho}$: weight matrices for the forget gate, input gate, cell candidate, and the output gate for $h_{t-1}$. If the number of features of the hidden state are $n$, then these matrices will be of shape $n \\times n$.\n",
    "\n",
    "- $b_{if}, b_{ii}, b_{ig}, b_{io}$: bias terms for the forget gate, input gate, cell candidate, and the output gate respectively. These are a vector of length $n$, size of the hidden dimension.\n",
    "\n",
    "- $b_{hf}, b_{hi}, b_{hg}, b_{ho}$: bias terms for the forget gate, input gate, cell candidate, and the output gate respectively. These are a vector of length $n$, size of the hidden dimension.\n",
    "\n",
    "- $W_y$: the weight matrix for the output layer. If there are $n$ hidden dimensions and $k$ classes, the matrix will be $k \\times n$.\n",
    "\n",
    "- $b_y$: the bias term for the output layer. This is of lenth $k$, where $k$ is the number of classes.\n",
    "\n",
    "- $f_t, i_t, \\tilde{c}_t, o_t$: the outputs of the forget gate, input gate, cell candidate, and output gate at time $t$, respectively. Each iteration is a vector of length $n$, where $n$ is the number of hidden dimensions.\n",
    "\n",
    "- $c_t$: the cell state at time $t$, which is a combination of the previous cell state and the current cell candidate, controlled by the forget gate and input gate. Each iteration is a vector of length $n$, where $n$ is the number of hidden dimensions. \n",
    "\n",
    "- $h_t$: This is a matrix of shape $t \\times n$, where $t$ is the number of timesteps and $n$ is the size of the hidden dimesion. A unique hidden state is produced for each timestep. The collection of these hidden states is what is used to produce the prediction for each unique frame.\n",
    "\n",
    "- ${y}_t$: the output vector of the predicted log probabilities for each frame. The output matrix will be of length $n /times t$.  Our model only takes into account the final timestep of each frame.\n",
    "\n",
    "- ${y}_T$: is the output vector of the final predicted log probabilities for each frame. If there are 10 timesteps per frame, then the 10th one will be considered.\n",
    "\n",
    "       **Note: We should try averaging these instead of taking the final output. It would be a matrix of length (n). However, it would take the average of the output over the ten timesteps. It would take from a (t X n) matrix. \n",
    "\n",
    "\n",
    "- $\\hat{y}_t$: is the prediction for that frame. There will be one output per unique frame. If each frame is broken into 10 timesteps or 100, there will still only be one output per frame.\n",
    "\n",
    "Reference: Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems (pp. 3104-3112). \\\n",
    "Code Reference:https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Package Documentation: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References - Paper model overview: http://www.bioinf.jku.at/publications/older/2604.pdf \\\n",
    "    -Paper above model is modeled after: https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf \\\n",
    "    -Explanation - This guy worked at the stats dept at UNC I believe. https://www.youtube.com/watch?v=YCzL96nL7j0&ab_channel=StatQuestwithJoshStarmer \\\n",
    "    -step by step: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create 3D tensors to be passed to the LSTM model.\n",
    "\n",
    "# Create X and y.\n",
    "X = filtered_normalized_firing_rates.drop(columns=['frame']).values\n",
    "y = filtered_normalized_firing_rates['frame'].values\n",
    "\n",
    "# Encode categorical target values\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Create train/test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Set sequence length and determine new number of samples\n",
    "seq_len = 10 # Set sequence length\n",
    "num_samples_train = X_train.shape[0] // seq_len\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# Determine the new number of samples.\n",
    "num_samples_test = X_test.shape[0] // seq_len\n",
    "\n",
    "# Reshape input to have 3 dimensions. (samples, timesteps, features/units)\n",
    "X_train = X_train[:num_samples_train*seq_len]  \n",
    "X_train = X_train.reshape(num_samples_train, seq_len, num_features)\n",
    "\n",
    "# Reshape output to match input\n",
    "y_train = y_train[:num_samples_train*seq_len]  # Make the total length divisible by seq_len\n",
    "y_train = y_train.reshape(num_samples_train, seq_len, 1)  # Keep as 2D tensor\n",
    "y_train = y_train[:, -1]  # Predicting a single value for each sequence\n",
    "\n",
    "# Reshape input to be 3D (samples, timesteps, features)\n",
    "X_test = X_test[:num_samples_test*seq_len]  # Make the total length divisible by seq_len\n",
    "X_test = X_test.reshape(num_samples_test, seq_len, num_features)\n",
    "\n",
    "# Reshape output to match input\n",
    "y_test = y_test[:num_samples_test*seq_len]  # Make the total length divisible by seq_len\n",
    "y_test = y_test.reshape(num_samples_test, seq_len, 1)  # Keep as 2D tensor\n",
    "y_test = y_test[:, -1]  # Predicting a single value for each sequence\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(f\"X_train type: {type(X_train)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test type: {type(X_test)}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Implement an LSTM model with 1 feature vector.\n",
    "\n",
    "# Reshape X data to be 3D. This is useful if passing a 4d tensor, where there is an extra dimension that represents different features of each node.\n",
    "# 4D X ,(batch_size, timesteps_per_frame, nodes, features), becomes 3D (batch_size, timesteps_per_frame, nodes * features)\n",
    "X_train = X_train.view(X_train.size(0), X_train.size(1), -1)\n",
    "X_test = X_test.view(X_test.size(0), X_test.size(1), -1)\n",
    "\n",
    "# LSTM Model Parameters\n",
    "input_dim = X_train.shape[-1]  # replace with the input dimension\n",
    "hidden_dim = 500 # replace with the hidden dimension\n",
    "layer_dim = 10 # replace with the layer dimension; usually between 1-3\n",
    "num_classes = X_train.shape[-1]\n",
    "output_dim = len(np.unique(filtered_normalized_firing_rates['frame'].values))  # replace with the output dimension\n",
    "\n",
    "# Training Parameters\n",
    "sequence_length = seq_len  # replace with the sequence length\n",
    "input_size = input_dim  # replace with the input size\n",
    "learning_rate = 0.01  # replace with the learning rate\n",
    "num_epochs = 100  # replace with the number of epochs\n",
    "batch_size = 64  # You can adjust this value according to your preference and memory capacity\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True) # LSTM layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) # Fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        X (array): Matrix consists of batch_size, time_steps_per_frame, num_nodes. X.shape = (B, T, N).\n",
    "        '''\n",
    "        # Initialize hidden and cell states. h0 and c0 respectively.\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # Forward propagate the LSTM.\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        # This means our prediction comes from the output of the final timestep of that frame.\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        \n",
    "        return out\n",
    "    \n",
    "# Create the LSTM model.\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim).to(device)\n",
    "'''\n",
    "print(input_dim)\n",
    "print(hidden_dim)\n",
    "print(layer_dim)\n",
    "print(output_dim)\n",
    "'''\n",
    "# Set the loss function and optimizer.\n",
    "criterion = nn.CrossEntropyLoss()  # use cross entropy loss function for multi-class classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Convert test data to tensor in preperation for the laoder.\n",
    "X_test_tensor = torch.Tensor(X_test).view(-1, sequence_length, input_dim).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "# Create DataLoaders for training and testing data.\n",
    "train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(list(zip(X_test_tensor, y_test_tensor)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set total of test samples.\n",
    "total_test_samples = len(X_test)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_running_loss = 0.0  # Initialize running loss to 0.0\n",
    "    correct_train_preds = 0.0  # Initialize the number of correct predictions to 0.0\n",
    "    total_train_samples = 0.0  # Initialize the total number of training samples to 0.0\n",
    "    correct = 0\n",
    "    # Training step\n",
    "    for i, (features, labels) in enumerate(train_loader):  # Loop over the training data\n",
    "        #print(i)\n",
    "        #print(type(features))\n",
    "        #print(np.shape(labels))\n",
    "        features = features.view(-1, sequence_length, input_dim).to(device)  # Reshape the features and move them to the device\n",
    "        labels = labels.to(device)  # Move the labels to the device\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(features)  # Pass the features through the model to get the output\n",
    "\n",
    "        # Flatten labels to a 1D tensor\n",
    "        labels = labels.view(-1)  # Flatten the labels\n",
    "        #print(np.shape(labels))\n",
    "        # If labels are one-hot encoded, convert to class indices\n",
    "        if labels.dim() > 1:  # If the labels are one-hot encoded\n",
    "            labels = torch.argmax(labels, dim=1)  # Convert to class indices\n",
    "\n",
    "        # Ensure labels are of the correct data type\n",
    "        labels = labels.long()  # Convert labels to long type\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(out, labels)  # Compute the loss between the output and the labels\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        loss.backward()  # Perform a backward pass to compute the gradients\n",
    "        optimizer.step()  # Perform an optimization step to update the parameters\n",
    "\n",
    "        train_running_loss += loss.detach().item()  # Add the loss to the running loss\n",
    "        _, preds = torch.max(out, dim=1)  # Get the predicted classes\n",
    "        correct_train_preds += (preds == labels).sum().item()  # Add the number of correct predictions to the total\n",
    "        total_train_samples += labels.shape[0]  # Add the number of labels to the total number of samples\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct_test_preds = 0.0  # Initialize the number of correct test predictions to 0.0\n",
    "    total_test_samples = 0.0  # Initialize the total number of test samples to 0.0\n",
    "    \n",
    "    # Test step\n",
    "    for i, (features, labels) in enumerate(test_loader):  # Loop over the test data\n",
    "        features = features.view(-1, sequence_length, input_dim).to(device)  # Reshape the features and move them to the device\n",
    "        labels = labels.to(device)  # Move the labels to the device\n",
    "        labels = labels.squeeze()  # Remove the extra dimension from the labels tensor\n",
    "        out = model(features)  # Pass the features through the model to get the output\n",
    "        _, preds = torch.max(out, dim=1)  # Get the predicted classes\n",
    "        correct = (preds == labels).sum().item()  # Compute the number of correct predictions\n",
    "        correct_test_preds += correct  # Add the number of correct predictions to the total\n",
    "        total_test_samples += labels.shape[0]  # Add the number of labels to the total number of samples\n",
    "\n",
    "\n",
    "    train_acc = correct_train_preds / total_train_samples * 100  # Compute the training accuracy\n",
    "    test_acc = correct_test_preds / total_test_samples * 100  # Compute the test accuracy\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {np.round(train_running_loss/i,2)}, Train Acc: {np.round(train_acc,2)}%, Test Acc: {np.round(test_acc,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFRcJQx_Xmxk",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.8) Build a Static Graph Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_encoded_torch = torch.tensor(y_encoded, dtype=torch.float)y_encoded_torch = torch.tensor(y_encoded, dtype=torch.float)3.6.1) Create the adjacency matrix.\n",
    " - Here we will define the adjacency matrix.\n",
    " - In this case, we will set our starting adjacency matrix to be a correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.7.1) Create the graph objects using correlation as the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "edge_threshold = 0.3\n",
    "\n",
    "# Create X and y.\n",
    "y = filtered_normalized_firing_rates['frame'].values\n",
    "X = filtered_normalized_firing_rates.drop(columns=['frame']).values\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Encode labels to class indices\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Define a correlation-based edge index\n",
    "corr_matrix = np.corrcoef(X, rowvar=False)\n",
    "\n",
    "# Trim edges by edge_threshold.\n",
    "edges = np.argwhere(corr_matrix > edge_threshold)\n",
    "\n",
    "# edge_index should be a 2xN tensor where N is the number of edges, so we transpose the result\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Split the edges into training and test sets\n",
    "num_edges = edge_index.shape[1]\n",
    "perm = torch.randperm(num_edges)\n",
    "train_perm = perm[:int(0.8 * num_edges)]  # 80% of edges for training\n",
    "test_perm = perm[int(0.8 * num_edges):]  # 20% of edges for testing\n",
    "\n",
    "train_edges = edge_index[:, train_perm]\n",
    "test_edges = edge_index[:, test_perm]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "num_samples = X.shape[0]\n",
    "perm = torch.randperm(num_samples)\n",
    "train_perm = perm[:int(0.8 * num_samples)]  # 80% of samples for training\n",
    "test_perm = perm[int(0.8 * num_samples):]  # 20% of samples for testing\n",
    "\n",
    "X_train = X[train_perm, :]\n",
    "y_train = y_encoded[train_perm]\n",
    "X_test = X[test_perm, :]\n",
    "y_test = y_encoded[test_perm]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create the Data objects\n",
    "train_data = Data(x=X_train_tensor, y=y_train_tensor, edge_index=train_edges)\n",
    "test_data = Data(x=X_test_tensor, y=y_test_tensor, edge_index=test_edges)\n",
    "\n",
    "\n",
    "train_data.num_classes = num_classes\n",
    "test_data.num_classes = num_classes\n",
    "\n",
    "\n",
    "print(f\"Edge Threshold: {edge_threshold}\")\n",
    "print(f\"X_train type: {type(X_train)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test type: {type(X_test)}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.7.2) Build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(train_data.num_features, 128)  # assuming number of features is data.num_features\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "        self.classifier = Linear(64, train_data.num_classes)  # assuming number of classes is data.num_classes\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First Convolutional layer with ReLU activation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Second Convolutional layer with ReLU activation\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Output layer\n",
    "        out = F.log_softmax(self.classifier(x), dim=1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(train_data.num_features, 700)  # assuming number of features is data.num_features\n",
    "        self.classifier = Linear(700, train_data.num_classes)  # assuming number of classes is data.num_classes\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First Convolutional layer with ReLU activation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Output layer\n",
    "        out = F.log_softmax(self.classifier(x), dim=1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.7.3) Train and Evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate the model and the optimizer\n",
    "model = GNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Initialize a list to store the accuracies\n",
    "test_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(50):  # assuming you want to train for 200 epochs\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    out = model(train_data.to(device))  # Perform forward pass\n",
    "    loss = F.nll_loss(out, train_data.y.to(device))  # Compute the loss\n",
    "    loss.backward()  # Perform backward pass\n",
    "    optimizer.step()  # Update the weights\n",
    "\n",
    "    # Switch model to evaluation mode for accuracy calculation\n",
    "    model.eval()\n",
    "\n",
    "    # Perform forward pass on your test data\n",
    "    out_test = model(test_data.to(device))\n",
    "\n",
    "    # Convert output probabilities to predicted class\n",
    "    _, preds_test = torch.max(out_test, dim=1)\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    correct_test = float((preds_test == test_data.y.to(device)).sum().item())\n",
    "    total_test = float(test_data.y.size(0))\n",
    "    accuracy_test = correct_test / total_test\n",
    "\n",
    "    # Append the accuracy to the list\n",
    "    test_accuracies.append(accuracy_test)\n",
    "\n",
    "    # Switch model back to training mode\n",
    "    model.train()\n",
    "\n",
    "# Plot the accuracies\n",
    "plt.plot(range(1, 51), test_accuracies)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Test Accuracy per Epoch')\n",
    "plt.show()\n",
    "# Instantiate the model and the optimizer\n",
    "model = GNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(15):  # assuming you want to train for 200 epochs\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    out = model(train_data.to(device))  # Perform forward pass\n",
    "    loss = F.nll_loss(out, train_data.y.to(device))  # Compute the loss\n",
    "    loss.backward()  # Perform backward pass\n",
    "    optimizer.step()  # Update the weights\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform forward pass on your training data\n",
    "out_train = model(train_data.to(device))\n",
    "\n",
    "# Convert output probabilities to predicted class\n",
    "_, preds_train = torch.max(out_train, dim=1)\n",
    "\n",
    "# Calculate training accuracy\n",
    "correct_train = float((preds_train == train_data.y.to(device)).sum().item())\n",
    "total_train = float(train_data.y.size(0))\n",
    "accuracy_train = correct_train / total_train\n",
    "\n",
    "# Print the training accuracy\n",
    "print(f\"Training Accuracy: {accuracy_train * 100:.2f}%\")\n",
    "\n",
    "# Perform forward pass on your test data\n",
    "out_test = model(test_data.to(device))\n",
    "\n",
    "# Convert output probabilities to predicted class\n",
    "_, preds_test = torch.max(out_test, dim=1)\n",
    "\n",
    "# Convert log probabilities to probabilities for test set\n",
    "out_prob_test = torch.exp(out_test)\n",
    "\n",
    "# Convert tensors to numpy arrays for use with sklearn\n",
    "out_prob_test_np = out_prob_test.cpu().detach().numpy()\n",
    "y_test_np = y_test_tensor.cpu().numpy()\n",
    "\n",
    "# Compute metrics for each class and average\n",
    "precision = precision_score(y_test_np, preds_test.cpu().numpy(), average='macro')\n",
    "recall = recall_score(y_test_np, preds_test.cpu().numpy(), average='macro')\n",
    "f1 = f1_score(y_test_np, preds_test.cpu().numpy(), average='macro')\n",
    "accuracy_test = correct_test / total_test\n",
    "\n",
    "# For AUC-ROC, we need to get the probabilities of each class\n",
    "roc_auc = roc_auc_score(y_test_np, out_prob_test_np, multi_class='ovr', average='macro')\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_test * 100:.2f}%\")\n",
    "print(f'Precision: {precision*100:.2f}%')\n",
    "print(f'Recall: {recall*100:.2f}%')\n",
    "print(f'F1 Score: {f1*100:.2f}%')\n",
    "print(f'ROC AUC: {roc_auc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.9) Build a Graph Attention Network(GAT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT documentation: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.10) Build a static ST-GNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Here one adjacency matrix is trained for the whole network. \n",
    "\n",
    "Friendly Introduction to Temporal Graph Neural Networks (and some Traffic Forecasting) by DeepFindr- https://www.youtube.com/watch?v=WEWq93tioC4&t=4s&ab_channel=DeepFindr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/FelixOpolka/STGCN-PyTorch/blob/master/stgcn.py\n",
    "- GAT: Veličković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., & Bengio, Y. (2018). Graph Attention Networks. In International Conference on Learning Representations (ICLR).\n",
    "-https://arxiv.org/pdf/1710.10903.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_MAPPING_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2136886/3840751980.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Create Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTGNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial_in_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_input_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCapturableAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# Flattens params (on CUDA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mflatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mnum_weights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     torch._cudnn_rnn_flatten_weight(\n\u001b[0m\u001b[1;32m    181\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cudnn_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_MAPPING_ERROR"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "os.environ['CUDNN_DISABLE_CUDAGRAPH'] = '1'\n",
    "\n",
    "# Set model tuning parameters.\n",
    "num_epochs = 100\n",
    "hidden_dim = 32\n",
    "layer_dim = 1\n",
    "batch_size = 64\n",
    "learning_rate = .001\n",
    "metrics_file = 'training_metrics_1.txt'\n",
    "\n",
    "# Parameters\n",
    "spatial_in_features = X_train.shape[3] # F from (B, T, N, F)\n",
    "output_dim = len(np.unique(y_train)) # Unique y values.\n",
    "num_nodes = np.shape(X)[1] # N from (B, T, N, F)\n",
    "lstm_input_dim = spatial_in_features * num_nodes # Changed this line\n",
    "edge_index = torch.tensor([[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j], dtype=torch.long).t().contiguous().to(device)\n",
    "\n",
    "class AdaptiveAdjacencyLayer(nn.Module):\n",
    "    def __init__(self, num_nodes, hidden_dim):\n",
    "        super(AdaptiveAdjacencyLayer, self).__init__()\n",
    "        self.V_Adap = nn.Parameter(torch.randn(num_nodes, num_nodes))  # Changing the shape to (num_nodes, num_nodes)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        A_Adap = self.activation(self.V_Adap) / self.V_Adap.size(0)\n",
    "        return torch.matmul(X, A_Adap)  # Correcting the order of multiplication\n",
    "\n",
    "class TrainableGATLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TrainableGATLayer, self).__init__()\n",
    "        self.gat_conv = GATConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.gat_conv(x, edge_index)\n",
    "    \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, X): # X: (batch_size, time_steps, flattened_nodes_and_features)\n",
    "        # Pass through LSTM\n",
    "        out, _ = self.lstm(X)\n",
    "        # Apply the fully connected layer (optional, based on your specific needs)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class STGNN(nn.Module):\n",
    "    def __init__(self, spatial_in_features, num_nodes, lstm_input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(STGNN, self).__init__()\n",
    "        self.adaptive_adjacency = AdaptiveAdjacencyLayer(num_nodes, hidden_dim)\n",
    "        self.gat_layer = TrainableGATLayer(spatial_in_features, spatial_in_features)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.lstm = LSTMModel(lstm_input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        spatial_out = []\n",
    "        B, T, NF = X.size()\n",
    "        # In the STGNN forward method\n",
    "        # Inside the STGNN forward method\n",
    "        for t in range(T):\n",
    "            x_t = X[:, t, :].view(B, self.num_nodes, spatial_in_features).squeeze(-1)  # Reshape to (B, N) and remove the last dimension\n",
    "            x_t_adj = [self.adaptive_adjacency(x) for x in x_t]\n",
    "            x_t_adj = torch.stack(x_t_adj)\n",
    "            spatial_out_t = self.gat_layer(x_t_adj.view(-1, spatial_in_features), edge_index)\n",
    "            spatial_out_t = spatial_out_t.view(B, self.num_nodes, spatial_in_features)\n",
    "            spatial_out.append(spatial_out_t)\n",
    "        spatial_out = torch.stack(spatial_out, dim=1)\n",
    "        spatial_out = spatial_out.view(B, T, -1)\n",
    "        # Pass through LSTM\n",
    "        out = self.lstm(spatial_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Create Model\n",
    "model = STGNN(spatial_in_features, num_nodes, lstm_input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model.to(device)\n",
    "\n",
    "class CapturableAdam(torch.optim.Adam):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CapturableAdam, self).__init__(*args, **kwargs)\n",
    "        self.defaults['capturable'] = True\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = CapturableAdam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(list(zip(X_train_reshaped, y_train)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(X_test_reshaped, y_test)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "import time\n",
    "# Save Parameters in file.\n",
    "with open(metrics_file, 'a') as file:\n",
    "        file.write(f'--------------------------------------------------------------------------------------------------------------------')\n",
    "        file.write(f'--------------------------------------------------------------------------------------------------------------------')\n",
    "        file.write(f'hidden_dim = {hidden_dim}, layer_dim = {layer_dim}, learning_rate = {learning_rate},and batch_size = {batch_size}.')\n",
    "        file.write(f'--------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time() # Record start time to be saved.\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        labels = labels.squeeze().long() # Squeeze the labels into 1D\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct_train / total_train\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            labels = labels.squeeze().long()\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = 100 * correct_test / total_test\n",
    "    \n",
    "    # Print Results\n",
    "    print(f'Epoch {epoch+1}, Loss: {np.round((running_loss / len(train_loader)),2)}, Train Acc: {np.round(train_acc, 2)}%, Test Acc: {np.round(test_acc, 2)}%')\n",
    "    \n",
    "    # Save Results\n",
    "    end_time = time.time() # Record end time\n",
    "    epoch_duration = end_time - start_time\n",
    "    # Save output to a txt file.\n",
    "    with open(metrics_file, 'a') as file:\n",
    "        file.write(f'Epoch {epoch+1}, Loss: {np.round((running_loss / len(train_loader)),2)}, Train Acc: {np.round(train_acc, 2)}%, Test Acc: {np.round(test_acc, 2)}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_adjacency_matrix = model.adaptive_adjacency.V_Adap.detach().cpu().numpy()\n",
    "A_Adap = 1 / (1 + np.exp(-adaptive_adjacency_matrix)) / adaptive_adjacency_matrix.shape[0]\n",
    "np.shape(A_Adap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.11) Dynamic STGNN with adjacency matrix for each frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our Static ST-GNN model we found a single directed adjacency matrix to represent the connections(edges) between the firing rates(nodes) of our network. In our Dynamic ST-GNN we will allow for a different adjacency matrix for each possible frame(class). Giving possible insights into the directed functional connectomics between neurons given different tasks.\n",
    "\n",
    "    G = (V, E_C, X_(N,T), X_E). Where C is classes. T is each unique frame. There will be 118 graphs produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TEMPORAL GRAPH NETWORKS FOR DEEP LEARNING ON DYNAMIC GRAPHS: https://arxiv.org/pdf/2006.10637.pdf\n",
    "-https://arxiv.org/pdf/2104.07788.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 67/67 [02:07<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.8, Train Acc: 0.54%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 67/67 [02:09<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 67/67 [02:08<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 67/67 [02:08<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 67/67 [02:08<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 67/67 [02:08<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n",
      "Early stopping triggered at epoch 6\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "# Set model tuning parameters.\n",
    "num_epochs = 40\n",
    "learning_rate = .001\n",
    "hidden_dim = 1000 #  Set the hidden dimensions for the LSTM\n",
    "layer_dim = 1 # Set the layer dimensions for the LSTM\n",
    "spatial_hidden_dim = 50 # Hidden dimension in GAT\n",
    "spatial_out_features = 1 # The input size to the LSTM will be this parameter multiplied by the number of nodes.\n",
    "edge_threshold = .5 # Drop all edge connections below threshold.\n",
    "metrics_file = 'training_metrics_1.txt'# Set file location for saving model output.\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "min_delta = 0.1\n",
    "best_val_acc = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Parameters\n",
    "spatial_in_features = X_train.shape[3] # F from (B, T, N, F)\n",
    "output_dim = len(np.unique(y_train)) # Unique y values.\n",
    "num_nodes = np.shape(X)[1] # N from (B, T, N, F)\n",
    "lstm_input_dim = spatial_in_features * num_nodes # Changed this line\n",
    "edge_index = torch.tensor([[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j], dtype=torch.long).t().contiguous().to(device)\n",
    "num_classes = len(np.unique(y_train))  # Unique y values\n",
    "\n",
    "class TrainableGATLayer(nn.Module):\n",
    "    def __init__(self, in_channels, spatial_hidden_dim, spatial_out_features):\n",
    "        super(TrainableGATLayer, self).__init__()\n",
    "        self.gat_conv = GATv2Conv(in_channels, spatial_hidden_dim)  # Initial GAT layer\n",
    "        self.fc = nn.Linear(spatial_hidden_dim, spatial_out_features)  # Fully Connected layer for reducing dimension\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        B, N, F = x.size()\n",
    "        x_reshaped = x.reshape(-1, F)\n",
    "        edge_index_repeated = edge_index.repeat(1, B)\n",
    "        # GAT layer\n",
    "        output = self.gat_conv(x_reshaped, edge_index_repeated)\n",
    "        # Fully Connected layer\n",
    "        output_reduced = self.fc(output)\n",
    "        return output_reduced.reshape(B, N, -1)\n",
    "\n",
    "class CapturableAdam(torch.optim.Adam):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CapturableAdam, self).__init__(*args, **kwargs)\n",
    "        self.defaults['capturable'] = True\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    # Future me, try adding a dropout layer.\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, X): # X: (batch_size, time_steps, flattened_nodes_and_features)\n",
    "        # Pass through LSTM\n",
    "        out, _ = self.lstm(X)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out \n",
    "\n",
    "class DynamicAdaptiveAdjacencyLayer(nn.Module):\n",
    "    def __init__(self, num_classes, num_nodes, hidden_dim):\n",
    "        super(DynamicAdaptiveAdjacencyLayer, self).__init__()\n",
    "        self.V_Adap = nn.Parameter(torch.randn(num_classes, num_nodes, num_nodes))\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, class_idx):\n",
    "        A_Adap = self.activation(self.V_Adap[class_idx]) / self.V_Adap.size(1)\n",
    "        edge_index = (A_Adap > edge_threshold).nonzero(as_tuple=False).t()\n",
    "        return edge_index\n",
    "\n",
    "class FullyDynamicSTGNN(nn.Module):\n",
    "    def __init__(self, spatial_in_features, num_classes, num_nodes, lstm_input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(FullyDynamicSTGNN, self).__init__()\n",
    "        self.dynamic_adjacency = DynamicAdaptiveAdjacencyLayer(num_classes, num_nodes, hidden_dim)\n",
    "        self.gat_layer = TrainableGATLayer(spatial_in_features, spatial_hidden_dim, spatial_out_features)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.lstm = LSTMModel(lstm_input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "    def forward(self, X, class_idx):\n",
    "        spatial_out = []\n",
    "        B, T, NF = X.size()\n",
    "        edge_index = self.dynamic_adjacency(class_idx) # Get dynamic edge index for class_idx\n",
    "        for t in range(T):\n",
    "            x_t = X[:, t, :].view(B, self.num_nodes, spatial_in_features)\n",
    "            spatial_out_t = self.gat_layer(x_t, edge_index)\n",
    "            spatial_out.append(spatial_out_t)\n",
    "        spatial_out = torch.stack(spatial_out, dim=1)\n",
    "        spatial_out = spatial_out.view(B, T, -1) \n",
    "        out = self.lstm(spatial_out)\n",
    "        return out\n",
    "    \n",
    "label_encoder = LabelEncoder().fit(y_train.ravel())\n",
    "\n",
    "# Assuming X_train is your training data with shape (B, T, N, F)\n",
    "num_time_steps = X_train.shape[1] # T from (B, T, N, F)\n",
    "\n",
    "# Model initialization\n",
    "model = FullyDynamicSTGNN(spatial_in_features, num_classes, num_nodes, lstm_input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = CapturableAdam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train() # Set the model to training mode\n",
    "    running_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for (features, labels) in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        labels = labels.squeeze().long()\n",
    "        class_idx = label_encoder.transform(labels.cpu().numpy())[0] # Transform labels to class indices\n",
    "        class_idx = torch.tensor(class_idx, dtype=torch.long).to(device)\n",
    "        outputs = model(features, class_idx)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct_train / total_train\n",
    "\n",
    "    # Test\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for (features, labels) in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            labels = labels.squeeze().long()\n",
    "            class_idx = label_encoder.transform(labels.cpu().numpy())[0] # Transform labels to class indices\n",
    "            class_idx = torch.tensor(class_idx, dtype=torch.long).to(device)\n",
    "            outputs = model(features, class_idx) # Call the model with the features and class index\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = 100 * correct_test / total_test\n",
    "    # Print Results\n",
    "    print(f'Epoch {epoch+1}, Loss: {np.round((running_loss / len(train_loader)),2)}, Train Acc: {np.round(train_acc, 2)}%, Test Acc: {np.round(test_acc, 2)}%')\n",
    "\n",
    "    # Save Results\n",
    "    end_time = time.time()  # Record end time\n",
    "    epoch_duration = end_time - start_time\n",
    "    # Save output to a txt file.\n",
    "    with open(metrics_file, 'a') as file:\n",
    "        file.write(f'Epoch {epoch+1}, Loss: {np.round((running_loss / len(train_loader)),2)}, Train Acc: {np.round(train_acc, 2)}%, Test Acc: {np.round(test_acc, 2)}%\\n')\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if test_acc - best_val_acc > min_delta:\n",
    "        best_val_acc = test_acc\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dynamic_adjacency_layer = model.dynamic_adjacency\n",
    "raw_adjacency_matrices = dynamic_adjacency_layer.V_Adap.detach().cpu().numpy()\n",
    "adjacency_matrices = 1 / (1 + np.exp(-raw_adjacency_matrices))\n",
    "#adjacency_matrices = (adjacency_matrices > 0.5).astype(int)\n",
    "\n",
    "#adjacency_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Your existing code to get adjacency_matrices\n",
    "# ... (your model training and evaluation code)\n",
    "dynamic_adjacency_layer = model.dynamic_adjacency\n",
    "raw_adjacency_matrices = dynamic_adjacency_layer.V_Adap.detach().cpu().numpy()\n",
    "adjacency_matrices = 1 / (1 + np.exp(-raw_adjacency_matrices))\n",
    "adjacency_matrices = (adjacency_matrices > 0.5).astype(int)\n",
    "\n",
    "# Create a new directory for the adjacency matrices if it doesn't exist\n",
    "adjacency_dir = \"adjacency_matrix\"\n",
    "if not os.path.exists(adjacency_dir):\n",
    "    os.makedirs(adjacency_dir)\n",
    "\n",
    "# Function to save the adjacency matrices\n",
    "def save_adjacency_matrices(adjacency_matrices, model_name):\n",
    "    \"\"\"\n",
    "    Save the adjacency matrices to pickle files.\n",
    "    \n",
    "    Parameters:\n",
    "    - adjacency_matrices: NumPy array containing the adjacency matrices\n",
    "    - model_name: Identifier for the current model\n",
    "    \"\"\"\n",
    "    # Determine the next available trial number for this model\n",
    "    trial_number = 1\n",
    "    while os.path.exists(os.path.join(adjacency_dir, f\"{model_name}_trial_{trial_number}\")):\n",
    "        trial_number += 1\n",
    "    \n",
    "    # Create a subfolder for the current model run and trial\n",
    "    run_dir = os.path.join(adjacency_dir, f\"{model_name}_trial_{trial_number}\")\n",
    "    os.makedirs(run_dir)\n",
    "    \n",
    "    total_matrices = len(adjacency_matrices)\n",
    "    \n",
    "    for i, adj_matrix in enumerate(adjacency_matrices):\n",
    "        # Create a unique filename for each adjacency matrix\n",
    "        filename = f\"adjacency_matrix_{i+1}_of_{total_matrices}.pkl\"\n",
    "        \n",
    "        # Full path to save the pickle file\n",
    "        filepath = os.path.join(run_dir, filename)\n",
    "        \n",
    "        # Save the adjacency matrix as a pickle file\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(adj_matrix, f)\n",
    "\n",
    "# Save the adjacency matrices\n",
    "save_adjacency_matrices(adjacency_matrices, 'model_1')\n",
    "print('saved adjacency matrices from model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_metrics(model_name, hidden_dims, layers, time_component, spatial_component,\n",
    "                       cross_mouse_ability, model_accuracy, timesteps_per_frame, num_epochs,\n",
    "                       learning_rate, spatial_hidden_dim, spatial_out_features, edge_threshold,\n",
    "                       csv_file='model_metrics.csv'):\n",
    "\n",
    "    # Create a DataFrame with the model metrics\n",
    "    data = {\n",
    "        'Model Name': [model_name],\n",
    "        'Model Accuracy': [model_accuracy],  # Moved to second position\n",
    "        'Hidden Dimensions': [hidden_dims],\n",
    "        'Layers': [layers],\n",
    "        'Time Component': [time_component],\n",
    "        'Spatial Component': [spatial_component],\n",
    "        'Cross Mouse Ability': [cross_mouse_ability],\n",
    "        'Timesteps per Frame': [timesteps_per_frame],\n",
    "        'Epochs': [num_epochs],\n",
    "        'Learning Rate': [learning_rate],\n",
    "        'Spatial Hidden Dim': [spatial_hidden_dim],\n",
    "        'Spatial Out Features': [spatial_out_features],\n",
    "        'Edge Threshold': [edge_threshold]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Check if the CSV file already exists\n",
    "    try:\n",
    "        existing_df = pd.read_csv(csv_file)\n",
    "    except FileNotFoundError:\n",
    "        existing_df = pd.DataFrame(columns=data.keys())\n",
    "    \n",
    "    # Append the new data and save it back to CSV\n",
    "    updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    updated_df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Parameters: Model Name, Hidden Dimensions, Layer Dimensions, Time Component, Spatial Component, Cross Mouse Ability, Model Accuracy, Timesteps per frame, 'Epochs' \n",
    "save_model_metrics('ST-GNN', hidden_dim, layer_dim, 'Yes', 'Yes', 'No', np.round(test_acc, 2), X_train.shape[1], num_epochs, learning_rate, spatial_hidden_dim, spatial_out_features, edge_threshold)\n",
    "print('Metrics saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "num_trials = 6  # Number of trials\n",
    "num_matrices = 118  # Number of matrices\n",
    "matrix_dim = 1141  # Dimension of the square adjacency matrix\n",
    "max_value = 1  # Replace with the maximum possible value in your adjacency matrices\n",
    "total_difference = 0\n",
    "\n",
    "for i in range(1, num_trials + 1):\n",
    "    for j in range(1, num_matrices + 1):\n",
    "        adj_matrix_trial_1 = load_adjacency_matrix('model_1', 1, j)\n",
    "        adj_matrix_trial_2 = load_adjacency_matrix('model_1', i, j)\n",
    "        \n",
    "        difference = norm(adj_matrix_trial_1 - adj_matrix_trial_2, 'fro')\n",
    "        total_difference += difference\n",
    "\n",
    "avg_difference = total_difference / (num_trials * num_matrices)\n",
    "max_possible_difference = np.sqrt(matrix_dim * matrix_dim * max_value ** 2 * (num_trials - 1))\n",
    "percentage_difference = (avg_difference / max_possible_difference) * 100\n",
    "\n",
    "print(f\"Average Frobenius norm of the differences: {np.round(avg_difference, 2)}\")\n",
    "print(f\"Percentage difference: {np.round(percentage_difference, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import math\n",
    "\n",
    "# Replace this with the absolute path to adjacency_matrix folder location.\n",
    "absolute_path_to_adjacency_matrix = '/proj/STOR/pipiras/Neuropixel/adjacency_matrix'\n",
    "\n",
    "def load_adjacency_matrix(model_name, trial_number, matrix_number):\n",
    "    run_dir = os.path.join(absolute_path_to_adjacency_matrix, f\"{model_name}_trial_{trial_number}\")\n",
    "    filepath = os.path.join(run_dir, f\"adjacency_matrix_{matrix_number}_of_118.pkl\")\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        adj_matrix = pickle.load(f)\n",
    "        \n",
    "    return adj_matrix\n",
    "\n",
    "def visualize_community(G, ax):\n",
    "    # Compute the best partition using Louvain algorithm\n",
    "    partition = community_louvain.best_partition(G)\n",
    "    \n",
    "    # Visualize the communities\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    cmap = plt.cm.get_cmap('viridis', max(partition.values()) + 1)\n",
    "    nx.draw_networkx_nodes(G, pos, partition.keys(), ax=ax, node_size=100, cmap=cmap, node_color=list(partition.values()))\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, alpha=0.5)\n",
    "    ax.set_title(f'Communities in Graph')\n",
    "\n",
    "def visualize_communities_across_trials(model_name, num_trials, num_to_visualize):\n",
    "    graphs = []\n",
    "    for i in range(1, num_trials + 1):\n",
    "        adj_matrix = load_adjacency_matrix(model_name, i, 1)  # Load the first graph of each trial\n",
    "        G = nx.from_numpy_matrix(adj_matrix)\n",
    "        graphs.append(G)\n",
    "    \n",
    "    n = min(len(graphs), num_to_visualize)\n",
    "    \n",
    "    # Create a figure and a grid of subplots\n",
    "    ncols = 2\n",
    "    nrows = math.ceil(n / ncols)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 12))\n",
    "\n",
    "    if nrows == 1:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for i, G in enumerate(graphs[:n]):\n",
    "        ax = axes.flatten()[i]\n",
    "        visualize_community(G, ax)\n",
    "        ax.set_title(f'Communities in Graph from Trial {i+1}')\n",
    "\n",
    "    for i in range(n, nrows * ncols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the number of graphs you want to visualize\n",
    "visualize_communities_across_trials('model_1', num_trials=5, num_to_visualize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import community  # pip install python-louvain\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def visualize_multiple_graphs(graph_list, ncols=3):\n",
    "    \"\"\"\n",
    "    Visualize multiple graphs side-by-side with community detection coloring.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph_list: List of NetworkX graphs to visualize\n",
    "    - ncols: Number of columns for the layout\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    n = len(graph_list)\n",
    "    nrows = math.ceil(n / ncols)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6 * ncols, 6 * nrows))\n",
    "    \n",
    "    # Flatten the axes array if there's only one row\n",
    "    if nrows == 1:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, G in enumerate(graph_list):\n",
    "        ax = axes.flatten()[i]\n",
    "        partition = community.best_partition(G)\n",
    "        pos = nx.spring_layout(G, seed=42)  # Using a seed for reproducibility\n",
    "        nx.draw(G, pos, ax=ax, with_labels=False, node_color=list(partition.values()), cmap=plt.cm.RdYlBu, node_size=100, alpha=0.8)\n",
    "        ax.set_title(f'Graph {i+1}')\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for i in range(n, nrows * ncols):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create some example NetworkX graphs (replace these with your actual graphs)\n",
    "G1 = nx.erdos_renyi_graph(100, 0.15)\n",
    "G2 = nx.erdos_renyi_graph(100, 0.20)\n",
    "G3 = nx.erdos_renyi_graph(100, 0.25)\n",
    "\n",
    "# Visualize the graphs\n",
    "visualize_multiple_graphs([G1, G2, G3], ncols=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.12) Dynamic STGNN with adjacency matrix for each timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    G = (V, E_C, X_(N,t), X_E). Where C is classes. t is each unique timestep. If 10 timesteps per frame, there will be 1180 seperate graphs. (118 classes * 10 timesteps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Play around more with the GAT. Try layering it instead of connecting the output with a neural net.\n",
    "Note: Play around with number of headss in the GAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 67/67 [01:17<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: nan, Train Acc: 0.85%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 67/67 [01:19<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 67/67 [01:19<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 67/67 [01:19<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 67/67 [01:19<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 67/67 [01:19<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 67/67 [01:19<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 67/67 [01:19<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 67/67 [01:19<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 67/67 [01:19<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 67/67 [01:19<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: nan, Train Acc: 0.94%, Test Acc: 0.85%\n",
      "Early stopping triggered at epoch 11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# Set model tuning parameters.\n",
    "num_epochs = 20\n",
    "spatial_hidden_dim = 10 # Hidden dimension in GAT\n",
    "spatial_out_features = 1 # The input size to the LSTM will be this parameter multiplied by the number of nodes.\n",
    "hidden_dim = 750 # Hidden dimension in LSTM\n",
    "layer_dim = 1\n",
    "batch_size = 64\n",
    "learning_rate = .001\n",
    "metrics_file = 'training_metrics_1.txt'\n",
    "\n",
    "# Parameters\n",
    "spatial_in_features = X_train.shape[3] # F from (B, T, N, F)\n",
    "output_dim = len(np.unique(y_train)) # Unique y values.\n",
    "num_nodes = np.shape(X)[1] # N from (B, T, N, F)\n",
    "lstm_input_dim = spatial_out_features * num_nodes  # N * D\n",
    "edge_index = torch.tensor([[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j], dtype=torch.long).t().contiguous().to(device)\n",
    "\n",
    "class TrainableGATLayer(nn.Module):\n",
    "    def __init__(self, in_channels, spatial_hidden_dim, spatial_out_features):\n",
    "        super(TrainableGATLayer, self).__init__()\n",
    "        self.gat_conv = GATConv(in_channels, spatial_hidden_dim)  # Initial GAT layer\n",
    "        self.fc = nn.Linear(spatial_hidden_dim, spatial_out_features)  # Fully Connected layer for reducing dimension\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        B, N, F = x.size()\n",
    "        x_reshaped = x.reshape(-1, F)\n",
    "        edge_index_repeated = edge_index.repeat(1, B)\n",
    "        \n",
    "        # GAT layer\n",
    "        output = self.gat_conv(x_reshaped, edge_index_repeated)\n",
    "        \n",
    "        # Fully Connected layer\n",
    "        output_reduced = self.fc(output)\n",
    "        \n",
    "        return output_reduced.reshape(B, N, -1)\n",
    "    \n",
    "class CapturableAdam(torch.optim.Adam):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CapturableAdam, self).__init__(*args, **kwargs)\n",
    "        self.defaults['capturable'] = True\n",
    "        \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, X): # X: (batch_size, time_steps, flattened_nodes_and_features)\n",
    "        # Pass through LSTM\n",
    "        out, _ = self.lstm(X)\n",
    "        # Apply the fully connected layer (optional, based on your specific needs)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out  # Added return statement here\n",
    "    \n",
    "class DynamicAdaptiveAdjacencyLayer(nn.Module):\n",
    "    def __init__(self, num_classes, num_time_steps, num_nodes):\n",
    "        super(DynamicAdaptiveAdjacencyLayer, self).__init__()\n",
    "        self.V_Adap = nn.Parameter(torch.randn(num_classes, num_time_steps, num_nodes, num_nodes))\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, class_idx, time_idx):\n",
    "        A_Adap = self.activation(self.V_Adap[class_idx, time_idx]) / self.V_Adap.size(3)\n",
    "        edge_index = (A_Adap > 0.5).nonzero(as_tuple=False).t()\n",
    "        return edge_index\n",
    "\n",
    "class FullyDynamicSTGNN(nn.Module):\n",
    "    def __init__(self, spatial_in_features, spatial_hidden_dim, num_classes, num_time_steps, num_nodes, lstm_input_dim, hidden_dim, layer_dim, output_dim, spatial_out_features):\n",
    "        super(FullyDynamicSTGNN, self).__init__()\n",
    "        self.dynamic_adjacency = DynamicAdaptiveAdjacencyLayer(num_classes, num_time_steps, num_nodes)\n",
    "        self.gat_layer = TrainableGATLayer(spatial_in_features, spatial_hidden_dim, spatial_out_features)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.lstm = LSTMModel(lstm_input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "    def forward(self, X, class_idx):\n",
    "        spatial_out = []\n",
    "        B, T, NF = X.size()\n",
    "        for t in range(T):\n",
    "            edge_index = self.dynamic_adjacency(class_idx, t) # Get dynamic edge index for class_idx and time step t\n",
    "            x_t = X[:, t, :].view(B, self.num_nodes, spatial_in_features)\n",
    "            spatial_out_t = self.gat_layer(x_t, edge_index)\n",
    "            spatial_out.append(spatial_out_t)\n",
    "        spatial_out = torch.stack(spatial_out, dim=1)\n",
    "        spatial_out = spatial_out.view(B, T, -1)\n",
    "        out = self.lstm(spatial_out)\n",
    "        return out\n",
    "    \n",
    "label_encoder = LabelEncoder().fit(y_train.ravel())\n",
    "\n",
    "# Parameters\n",
    "num_classes = len(np.unique(y_train))  # Unique y values\n",
    "\n",
    "# Assuming X_train is your training data with shape (B, T, N, F)\n",
    "num_time_steps = X_train.shape[1] # T from (B, T, N, F)\n",
    "\n",
    "# Model initialization\n",
    "model = FullyDynamicSTGNN(spatial_in_features, spatial_hidden_dim, num_classes, num_time_steps, num_nodes, lstm_input_dim, hidden_dim, layer_dim, output_dim, spatial_out_features)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = CapturableAdam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(list(zip(X_train_reshaped, y_train)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(X_test_reshaped, y_test)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "true_labels = []\n",
    "predicted_probs = []\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "min_delta = 0.1\n",
    "best_val_acc = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train() # Set the model to training mode\n",
    "    running_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for (features, labels) in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        labels = labels.squeeze().long()\n",
    "        class_idx = label_encoder.transform(labels.cpu().numpy())[0] # Transform labels to class indices\n",
    "        class_idx = torch.tensor(class_idx, dtype=torch.long).to(device)\n",
    "        outputs = model(features, class_idx)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct_train / total_train\n",
    "\n",
    "    # Test\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for (features, labels) in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            labels = labels.squeeze().long()\n",
    "            class_idx = label_encoder.transform(labels.cpu().numpy())[0]  # Transform labels to class indices\n",
    "            class_idx = torch.tensor(class_idx, dtype=torch.long).to(device)\n",
    "            outputs = model(features, class_idx)  # Call the model with the features and class index\n",
    "\n",
    "            # Store true labels and predicted probabilities\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_probs.extend(torch.softmax(outputs, dim=1).cpu().detach().numpy()[:, 1])  # Assuming the positive class is at index 1\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate AUC-ROC and F1 only after the final epoch\n",
    "    if epochs_no_improve == patience or epoch == num_epochs - 1:  # Add this condition to only compute at the final epoch\n",
    "        auc_roc = roc_auc_score(true_labels, predicted_probs)\n",
    "        f1 = f1_score(true_labels, np.round(predicted_probs))\n",
    "        print(f'Final AUC-ROC: {auc_roc}, Final F1-Score: {f1}')\n",
    "\n",
    "    test_acc = 100 * correct_test / total_test\n",
    "    # Print Results\n",
    "    print(f'Epoch {epoch+1}, Loss: {np.round((running_loss / len(train_loader)),2)}, Train Acc: {np.round(train_acc, 2)}%, Test Acc: {np.round(test_acc, 2)}%')\n",
    "\n",
    "    # Save Results\n",
    "    end_time = time.time()  # Record end time\n",
    "    epoch_duration = end_time - start_time\n",
    "    # Save output to a txt file.\n",
    "    with open(metrics_file, 'a') as file:\n",
    "        file.write(f'Epoch {epoch+1}, Loss: {np.round((running_loss / len(train_loader)),2)}, Train Acc: {np.round(train_acc, 2)}%, Test Acc: {np.round(test_acc, 2)}%\\n')\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if test_acc - best_val_acc > min_delta:\n",
    "        best_val_acc = test_acc\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_metrics(model_name, hidden_dims, layers, time_component, spatial_component,\n",
    "                       cross_mouse_ability, model_accuracy, auc_roc, f1_score, timesteps_per_frame, \n",
    "                       csv_file='model_metrics.csv'):\n",
    "\n",
    "    # Create a DataFrame with the model metrics\n",
    "    data = {\n",
    "        'Model Name': [model_name],\n",
    "        'Hidden Dimensions': [hidden_dims],\n",
    "        'Layers': [layers],\n",
    "        'Time Component': [time_component],\n",
    "        'Spatial Component': [spatial_component],\n",
    "        'Cross Mouse Ability': [cross_mouse_ability],\n",
    "        'Model Accuracy': [model_accuracy],\n",
    "        'AUC-ROC': [auc_roc],\n",
    "        'F1-Score': [f1_score],\n",
    "        'Timesteps per Frame': [timesteps_per_frame],\n",
    "        'Epochs': [num_epochs]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Check if the CSV file already exists\n",
    "    try:\n",
    "        existing_df = pd.read_csv(csv_file)\n",
    "    except FileNotFoundError:\n",
    "        existing_df = pd.DataFrame(columns=data.keys())\n",
    "    \n",
    "    # Append the new data and save it back to CSV\n",
    "    updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    updated_df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Parameters: Model Name, Hidden Dimensions, Layer Dimensions, Time Component, Spatial Component, Cross Mouse Ability, Model Accuracy, AUC-ROC, F1, Timesteps per frame, 'Epochs' \n",
    "save_model_metrics('ST-GNN', hidden_dim,  layer_dim, 'Yes', 'Yes', 'No', np.round(test_acc, 2),  auc_roc,  f1,  X_train.shape[1], {epoch+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dynamic_adjacency_layer is an instance of DynamicAdaptiveAdjacencyLayer\n",
    "dynamic_adjacency_layer = model.dynamic_adjacency\n",
    "\n",
    "# Retrieve the raw V_Adap tensor\n",
    "raw_adjacency_matrices = dynamic_adjacency_layer.V_Adap.detach().cpu().numpy()\n",
    "\n",
    "# Apply the sigmoid activation and thresholding\n",
    "adjacency_matrices = 1 / (1 + np.exp(-raw_adjacency_matrices))\n",
    "adjacency_matrices = (adjacency_matrices > 0.5).astype(int)\n",
    "\n",
    "# Now, adjacency_matricesis a NumPy array of shape (num_classes, num_time_steps, num_nodes, num_nodes)\n",
    "# containing all 1180 adjacency matrices\n",
    "np.shape(raw_adjacency_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.x) Try the embedding to transform data to be used across them mice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create an embedding that is the same length and has the same representation across mice to be able to concatanate mouse data. \n",
    "\n",
    "1. We need to create a binary dataset where rows are the timesteps(t) and columns are the stimuli shown(k = images shown) that indicates which image is shown.\n",
    "2. We need to multiply the transpose of our spike_df(t, n = neurons) with the binary(t, k), to produce a weighted matrix(n, k).\n",
    "3. To bring it to filtered_normalized_pickleshape (k,t) we need to multly our original spike_df(t, n) by our weighted matrix (n , k) to produce an output of (t,k).\n",
    "4. Concatenate the output with the other mice.\n",
    "5. Run on models.\n",
    "\n",
    "This output is the same for all mice. In our case it should be 118 if we only use natural images. The mice data should be able to be concatenated together.\n",
    "\n",
    "Transformation found here: https://arxiv.org/ftp/arxiv/papers/1911/1911.05479.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(frame_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "spike_df = spike_df[spike_df['frame'] != -1]\n",
    "\n",
    "# Get the 'frame' column from spike_df\n",
    "frame_indices = spike_df['frame'].values\n",
    "\n",
    "# One-hot encode the frame indices to create S\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "frame_indices = frame_indices.reshape(-1, 1)  # Reshape to make it a 2D array\n",
    "S = encoder.fit_transform(frame_indices)\n",
    "\n",
    "# Now S is your one-hot encoded stimulus matrix\n",
    "print(\"Shape of S:\", S.shape)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_df = spike_df[spike_df['frame'] != -1]\n",
    "R_df = spike_df.drop(columns=['frame'])\n",
    "R = R_df.to_numpy()\n",
    "np.shape(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('preprocessed_scenes shape =',np.shape(preprocessed_scenes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "spike_df = spike_df[spike_df['frame'] != -1]\n",
    "R_df = spike_df.drop(columns=['frame'])\n",
    "R = R_df.to_numpy()\n",
    "\n",
    "# Assuming scenes is a numpy array with shape (num_images, height, width)\n",
    "num_images, height, width = scenes.shape\n",
    "\n",
    "# Initialize a new array to hold the preprocessed images\n",
    "preprocessed_scenes = np.zeros((num_images, 224, 224, 3))\n",
    "\n",
    "for i in range(num_images):\n",
    "    # Resize to 224x224\n",
    "    resized_img = resize(scenes[i], (224, 224))\n",
    "    \n",
    "    # Duplicate across three channels\n",
    "    preprocessed_scenes[i] = np.stack([resized_img]*3, axis=2)\n",
    "\n",
    "# If you're using PyTorch, you'll also want to move the channel dimension to the front\n",
    "preprocessed_scenes = np.transpose(preprocessed_scenes, (0, 3, 1, 2))\n",
    "\n",
    "# Convert to PyTorch tensor if you're using PyTorch\n",
    "preprocessed_scenes = torch.tensor(preprocessed_scenes, dtype=torch.float32)\n",
    "\n",
    "# Now S is your one-hot encoded stimulus matrix\n",
    "print(\"Shape of preprocessed_scenes:\", preprocessed_scenes.shape)\n",
    "# Step 1: Preprocessing is already done; preprocessed_scenes has shape (118, 3, 224, 224)\n",
    "# Convert PyTorch tensor back to numpy for processing\n",
    "preprocessed_scenes_np = preprocessed_scenes.numpy()\n",
    "\n",
    "# Convert PyTorch tensor back to numpy for processing\n",
    "preprocessed_scenes_np = preprocessed_scenes.numpy()\n",
    "\n",
    "# Initialize S with zeros\n",
    "S = np.zeros((len(frame_indices), 3*224*224))\n",
    "\n",
    "# Populate S\n",
    "for i, scene_idx in enumerate(frame_indices):\n",
    "    # Ensure scene_idx is an integer\n",
    "    scene_idx = int(scene_idx)\n",
    "    \n",
    "    # Flatten the corresponding scene\n",
    "    flattened_scene = preprocessed_scenes_np[scene_idx].flatten()\n",
    "    \n",
    "    # Assign to S\n",
    "    S[i, :] = flattened_scene\n",
    "\n",
    "print('shape of S:',np.shape(S))\n",
    "'''\n",
    "# Step 3: Create Neural Response Matrix R\n",
    "# Assuming R is already created and has the same number of rows as S\n",
    "\n",
    "# Step 4: Calculate psi and phi\n",
    "psi = np.dot(R.T, S)\n",
    "phi = np.dot(R, psi)\n",
    "\n",
    "# Step 5: Create Training and Test Datasets\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(phi, dtype=torch.float32)\n",
    "y = torch.tensor(frame_labels, dtype=torch.long)  # frame_labels should correspond to each trial\n",
    "\n",
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"X_train type: {type(X_train)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test type: {type(X_test)}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'S_{session_number}.pkl', 'wb') as f:\n",
    "    pickle.dump(S, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'frame' column to get only the spike counts for each neuron\n",
    "spike_df = spike_df[spike_df['frame'] != -1]\n",
    "R_df = spike_df.drop(columns=['frame'])\n",
    "R = R_df.to_numpy()\n",
    "print('R Shape:',np.shape(R))\n",
    "print('S Shape:',np.shape(S))\n",
    "# Now proceed with calculating psi and phi as before\n",
    "psi = np.dot(R.T, S)\n",
    "phi = np.dot(R, psi)\n",
    "\n",
    "# Create X and y.\n",
    "y = spike_df['frame'].values\n",
    "X = phi \n",
    "\n",
    "# Encode class labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Create train/test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42, shuffle = False)\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "input_dim = X_train.shape[-1]\n",
    "\n",
    "\n",
    "print(f\"X_train type: {type(X_train)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test type: {type(X_test)}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load pre-trained GoogleNet\n",
    "model = models.googlenet(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 118)  # Assuming 118 different stimuli\n",
    "\n",
    "# 2. Prepare the Data\n",
    "# Assuming preprocessed_scenes is your input and y are the labels\n",
    "X = preprocessed_scenes\n",
    "y = torch.tensor(spike_df['frame'].values, dtype=torch.long)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# 3. Fine-Tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):  # 10 epochs, you can change this\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 4. Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {(100 * correct / total):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate psi (preferred stimuli for each neuron)\n",
    "psi = np.dot(R.T, S)\n",
    "\n",
    "# Initialize an empty list to store phi values\n",
    "phi_list = []\n",
    "\n",
    "# Calculate phi for each time step\n",
    "for t in range(R.shape[0]):\n",
    "    R_t = R[t, :]\n",
    "    phi_t = np.dot(R_t, psi)  # This is now a weighted sum, not a simple dot product\n",
    "    phi_list.append(phi_t)\n",
    "\n",
    "# Convert list of phi values to a numpy array\n",
    "phi = np.array(phi_list)\n",
    "\n",
    "# Create X and y.\n",
    "y = spike_df['frame'].values\n",
    "X = phi  # Now, X is a weighted sum of psi, which are the preferred stimuli\n",
    "\n",
    "# Encode class labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Show shapes and types for verification\n",
    "print(f\"X_train type: {type(X_train)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test type: {type(X_test)}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_df = filtered_normalized_firing_rates.copy()\n",
    "\n",
    "# Explicitly convert 'frame' to a numeric type, for example, to integer\n",
    "spike_df = spike_df[spike_df['frame'] != -1]\n",
    "spike_df['frame'] = spike_df['frame'].astype(int)\n",
    "\n",
    "# Drop the 'frame' column to get only the spike counts for each neuron\n",
    "R_df = spike_df.drop(columns=['frame'])\n",
    "R = R_df.to_numpy()\n",
    "\n",
    "# Now proceed with calculating psi and phi as before\n",
    "psi = np.dot(R.T, S)\n",
    "phi = np.dot(R, psi)\n",
    "\n",
    "# Create X and y.\n",
    "y = spike_df['frame'].values\n",
    "X = phi \n",
    "\n",
    "# Encode class labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Create train/test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42, shuffle = False)\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "input_dim = X_train.shape[-1]\n",
    "\n",
    "\n",
    "print(f\"X_train type: {type(X_train)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test type: {type(X_test)}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def generate_hidden_dims(top_layer_size, num_layers):\n",
    "    return [top_layer_size // (2 ** i) for i in range(num_layers)]\n",
    "\n",
    "# Define the model as a class\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes):\n",
    "        super(DeepNet, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dims[-1], num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 118  # Number of input features (neurons)\n",
    "top_layer_size = 2000  # Size of the top hidden layer\n",
    "num_layers = 8  # Number of hidden layers\n",
    "hidden_dims = generate_hidden_dims(top_layer_size, num_layers)\n",
    "num_classes = len(torch.unique(y_train))  # Number of output classes\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "learning_rate = .1\n",
    "\n",
    "# Create DataLoaders\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "model = DeepNet(input_dim, hidden_dims, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "\n",
    "print(f'Model Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Reshape your data to fit the input shape of GoogleNet (3x224x224)\n",
    "# For this example, we'll duplicate your 1D data across all three RGB channels and upscale its dimensions\n",
    "# Note: You may want to use more sophisticated resizing methods based on your specific data\n",
    "X_train_resized = X_train.view(-1, 1, 1).repeat(1, 3, 224*224)\n",
    "X_test_resized = X_test.view(-1, 1, 1).repeat(1, 3, 224*224)\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(X_train_resized, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_data = TensorDataset(X_test_resized, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pre-trained GoogleNet model + higher level Fully Connected\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "# Fine-tuning: remove classification head and add our custom one\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4) Overview of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of 4 is to create a table that allows one to compare each model and their parameters side by side and visulize the found network.\n",
    "\n",
    "- 4.1) Graph Model Metrics\n",
    "- 4.2) Visualize Found Graph Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1) Graph Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Model Name': ['Baseline', 'Graph Neural Net', 'ST-GNN'],\n",
    "    'Accuracy in Percent':['0.85','70','80'],\n",
    "    'Time Component': ['no', 'no', 'yes'],\n",
    "    'Spatial Component': ['no', 'yes', 'yes'],\n",
    "    'Hidden Dimensions': [0, 0, 0],\n",
    "    'Layers': [0, 0, 0],\n",
    "    'Cross Mouse Ability': [0, 0, 0],\n",
    "    'Model Accuracy': [0, 0, 0],\n",
    "    'AUC-ROC': [0, 0, 0],\n",
    "    'F1-Score': [0, 0, 0],\n",
    "    'Timesteps per Frame': [10, 10, 10],\n",
    "    'Preferred Stimulus Weight':['no','no','no']\n",
    "}\n",
    "grouped_df_for_latex = pd.DataFrame(data)\n",
    "\n",
    "# Create a new figure\n",
    "fig, ax = plt.subplots(figsize=(12, 4), frameon=False) # set the size that you'd like (width, height)\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create table and set the fontsize and position\n",
    "table = ax.table(cellText=grouped_df_for_latex.values,\n",
    "                 colLabels=grouped_df_for_latex.columns,\n",
    "                 cellLoc = 'center', \n",
    "                 loc='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(2.0, 2.0)\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Survey of Models in Predicting Visual Stimulus\", fontsize=16, y=.8)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UikqgS7gGqOH",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4.2) Build a graph for the network at each timestep. (Might need code to visualize later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help resources: \n",
    "- https://pytorch-geometric-temporal.readthedocs.io/en/latest/\n",
    "- https://github.com/topics/spatio-temporal-analysis?l=python\n",
    "- https://analyticsindiamag.com/a-beginners-guide-to-spatio-temporal-graph-neural-networks/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(\"graphs_721123822\"):\n",
    "    os.makedirs(\"graphs_721123822\")\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = np.corrcoef(filtered_normalized_firing_rates.drop(columns=['frame']).T)\n",
    "\n",
    "def create_graph(graph_index):\n",
    "    # Create a graph for this timestep\n",
    "    G = nx.Graph()\n",
    "    firing_rates = filtered_normalized_firing_rates.iloc[graph_index, 1:]\n",
    "    # Add nodes\n",
    "    G.add_nodes_from(filtered_normalized_firing_rates.columns[1:])\n",
    "    \n",
    "    for neuron, rate in zip(filtered_normalized_firing_rates.columns[1:], firing_rates):\n",
    "        G.add_node(neuron, rate=rate)\n",
    "\n",
    "    # Add edges\n",
    "    for i, neuron1 in enumerate(filtered_normalized_firing_rates.columns[1:]):\n",
    "        for j, neuron2 in enumerate(filtered_normalized_firing_rates.columns[1:]):\n",
    "            if i != j and correlations[i, j] > 0.25:  # Only add an edge if the correlation is above 0.5\n",
    "                G.add_edge(neuron1, neuron2, weight=correlations[i, j])\n",
    "    \n",
    "    # Add y variable as graph attribute\n",
    "    G.graph['frame'] = filtered_normalized_firing_rates.iloc[graph_index, 0]  # Assuming 'frame' is the first column\n",
    "\n",
    "    return G, graph_index\n",
    "\n",
    "def save_graph(G, graph_index):\n",
    "    # Determine the folder based on the graph_index\n",
    "    subfolder_name = f\"graphs_721123822_{graph_index // 10000}\"\n",
    "    \n",
    "    # Create the main directory if it doesn't exist\n",
    "    main_folder = \"graphs_721123822\"\n",
    "    if not os.path.exists(main_folder):\n",
    "        os.makedirs(main_folder)\n",
    "    \n",
    "    # Create the subdirectory inside the main directory\n",
    "    folder_name = os.path.join(main_folder, subfolder_name)\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Save the graph to a file in the \"graphs\" directory\n",
    "    with open(os.path.join(folder_name, f\"graph_{graph_index}.pkl\"), 'wb') as f:  # Save in pickle format\n",
    "        pickle.dump(G, f)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 200\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = len(filtered_normalized_firing_rates) // batch_size + 1\n",
    "\n",
    "# Create a ProcessPoolExecutor for graph creation and a ThreadPoolExecutor for I/O\n",
    "with ProcessPoolExecutor(max_workers=2) as executor, ThreadPoolExecutor(max_workers=2) as io_executor:\n",
    "    # Create a tqdm progress bar\n",
    "    pbar = tqdm(total=num_batches)\n",
    "    # Loop over each batch\n",
    "    error_indices = []  # List to keep track of graphs that failed to be created or saved\n",
    "    for batch in range(num_batches):\n",
    "        # Create a list to hold the futures\n",
    "        futures = []\n",
    "        start_index = batch * batch_size\n",
    "        end_index = (batch + 1) * batch_size\n",
    "        # Use the executor to submit the create_graph function for each timestep in the batch\n",
    "        for i in range(start_index, min(end_index, len(filtered_normalized_firing_rates))):\n",
    "            try:\n",
    "                futures.append(executor.submit(create_graph, i))\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Error creating graph {i}: {e}\")  # Use tqdm.write instead of print\n",
    "                error_indices.append(i)  # Append the index to the error list\n",
    "\n",
    "        # Wait for all futures to complete for this batch\n",
    "        for future in as_completed(futures):\n",
    "            # Use the io_executor to submit the save_graph function\n",
    "            try:\n",
    "                G, i = future.result()\n",
    "                io_executor.submit(save_graph, G, i)\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Error saving graph {i}: {e}\")  # Use tqdm.write instead of print\n",
    "                error_indices.append(i)  # Append the index to the error list\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "    # Close the progress bar\n",
    "    pbar.close()\n",
    "\n",
    "tqdm.write(\"Indices of graphs that failed to be created or saved:\", error_indices)  # Use tqdm.write instead of print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3D realistic neuron visualizations and wiggly graph(dude's a computational neuroscientist): https://www.youtube.com/watch?v=yaa13eehgzo&ab_channel=ArtemKirsanov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def generate_graphs(num_graphs, session_number):\n",
    "    # Create the output folder based on the session number\n",
    "    output_folder = f\"graph_visualizations_{session_number}\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # List to store the file paths of the images\n",
    "    image_file_paths = []\n",
    "\n",
    "    for i in range(num_graphs):\n",
    "        # Determine the subdirectory based on the graph index\n",
    "        subdirectory = f\"graphs_721123822_{i // 10000}\"\n",
    "\n",
    "        # Load the graph from the file\n",
    "        with open(os.path.join(\"graphs_721123822\", subdirectory, f\"graph_{i}.pkl\"), 'rb') as f:\n",
    "            G = pickle.load(f)\n",
    "\n",
    "        # Create a new figure\n",
    "        plt.figure(figsize=(10, 10))\n",
    "\n",
    "        # Get firing rates\n",
    "        rates = {n: d['rate'] for n, d in G.nodes(data=True)}\n",
    "\n",
    "        # Map firing rates to node sizes (you can adjust the scaling factor as needed)\n",
    "        node_sizes = [50 * rate for rate in rates.values()]\n",
    "\n",
    "        # Normalize firing rates to the range 0 to 1 for color mapping\n",
    "        max_rate = max(rates.values())\n",
    "        normalized_rates = [rate / max_rate for rate in rates.values()]\n",
    "\n",
    "        # Map normalized firing rates to colors using a color map\n",
    "        node_colors = [cm.hot(rate) for rate in normalized_rates]\n",
    "\n",
    "        # Create a list of edges to remove\n",
    "        edges_to_remove = [(u, v) for u, v in G.edges() if rates[u] <= 0 and rates[v] <= 0]\n",
    "\n",
    "        # Remove the edges\n",
    "        G.remove_edges_from(edges_to_remove)\n",
    "\n",
    "        # Get edge weights\n",
    "        edge_weights = [d['weight'] for u, v, d in G.edges(data=True)]\n",
    "\n",
    "        # Map edge weights to colors using a color map\n",
    "        edge_colors = [cm.cool(weight) for weight in edge_weights]\n",
    "\n",
    "        # Draw the graph as a circle\n",
    "        plt.gca().set_facecolor('black')\n",
    "        pos = nx.circular_layout(G)\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.7)\n",
    "        nx.draw_networkx_edges(G, pos, edge_color=edge_colors, alpha=1)\n",
    "\n",
    "        # Save the figure to a file in the output folder\n",
    "        image_file_path = os.path.join(output_folder, f\"graph_{i}.png\")\n",
    "        plt.savefig(image_file_path, facecolor='black')\n",
    "        image_file_paths.append(image_file_path)\n",
    "\n",
    "        # Clear the figure so that the next graph is not drawn on top of this one\n",
    "        plt.clf()\n",
    "\n",
    "    # Sort the image file paths numerically based on the index value\n",
    "    image_file_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split('_')[1].split('.')[0]))\n",
    "\n",
    "    # Create a GIF with a dynamic file name\n",
    "    gif_file_path = f\"graph_sequence_{num_graphs}.gif\"\n",
    "\n",
    "    # Create a GIF\n",
    "    with imageio.get_writer(gif_file_path, mode='I', loop=0) as writer:\n",
    "        for image_file_path in image_file_paths:\n",
    "            image = imageio.imread(image_file_path)\n",
    "            writer.append_data(image)\n",
    "\n",
    "    # Display the GIF\n",
    "    display(Image(filename=gif_file_path))\n",
    "    \n",
    "generate_graphs(num_graphs=100, session_number=721123822)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# List to store the file paths of the images\n",
    "image_file_paths = []\n",
    "num_graphs = 10\n",
    "for i in range(num_graphs):\n",
    "        # Determine the subdirectory based on the graph index\n",
    "        subdirectory = f\"graphs_721123822_{i // 10000}\"\n",
    "        # Load the graph from the file\n",
    "        with open(os.path.join(\"graphs_721123822\", subdirectory, f\"graph_{i}.pkl\"), 'rb') as f:\n",
    "            G = pickle.load(f)\n",
    "\n",
    "        # Create a new figure\n",
    "        plt.figure(figsize=(10, 10))\n",
    "\n",
    "        # Get firing rates\n",
    "        rates = {n: d['rate'] for n, d in G.nodes(data=True)}\n",
    "\n",
    "        # Map firing rates to node sizes (you can adjust the scaling factor as needed)\n",
    "        node_sizes = [50 * rate for rate in rates.values()]\n",
    "\n",
    "        # Normalize firing rates to the range 0 to 1 for color mapping\n",
    "        max_rate = max(rates.values())\n",
    "        normalized_rates = [rate / max_rate for rate in rates.values()]\n",
    "\n",
    "        # Map normalized firing rates to colors using a color map\n",
    "        node_colors = [cm.hot(rate) for rate in normalized_rates]\n",
    "\n",
    "        # Create a list of edges to remove\n",
    "        edges_to_remove = [(u, v) for u, v in G.edges() if rates[u] <= 0 and rates[v] <= 0]\n",
    "\n",
    "        # Remove the edges\n",
    "        G.remove_edges_from(edges_to_remove)\n",
    "\n",
    "        # Get edge weights\n",
    "        edge_weights = [d['weight'] for u, v, d in G.edges(data=True)]\n",
    "\n",
    "        # Map edge weights to colors using a color map\n",
    "        edge_colors = [cm.cool(weight) for weight in edge_weights]\n",
    "\n",
    "        # Draw the graph\n",
    "        plt.gca().set_facecolor('black')\n",
    "        pos = nx.kamada_kawai_layout(G)\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.7)\n",
    "        nx.draw_networkx_edges(G, pos, edge_color=edge_colors, alpha=1)\n",
    "\n",
    "        # Save the figure to a file\n",
    "        image_file_path = f\"graph_{i}.png\"\n",
    "        plt.savefig(image_file_path, facecolor='black')\n",
    "        image_file_paths.append(image_file_path)\n",
    "\n",
    "        # Clear the figure so that the next graph is not drawn on top of this one\n",
    "        plt.clf()\n",
    "\n",
    "# Sort the image file paths numerically based on the index value\n",
    "image_file_paths.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "\n",
    "# Create a GIF\n",
    "with imageio.get_writer('graph_sequence.gif', mode='I', loop=0) as writer:\n",
    "    for image_file_path in image_file_paths:\n",
    "        image = imageio.imread(image_file_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "# Display the GIF\n",
    "display(Image(filename='graph_sequence.gif'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"graphs_721123822/graphs_721123822_0/graph_10.pkl\"), 'rb') as f:\n",
    "    pickle.load(\"graph_10.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def generate_graphs(num_graphs, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # List to store the file paths of the images\n",
    "    image_file_paths = []\n",
    "\n",
    "    for i in range(num_graphs):\n",
    "        # Determine the subdirectory based on the graph index\n",
    "        subdirectory = f\"graphs_721123822_{i // 10000}\"\n",
    "\n",
    "        # Load the graph from the file\n",
    "        with open(os.path.join(\"graphs_721123822\", subdirectory, f\"graph_{i}.pkl\"), 'rb') as f:\n",
    "            G = pickle.load(f)\n",
    "\n",
    "        # Create a new figure\n",
    "        plt.figure(figsize=(10, 10))\n",
    "\n",
    "        # Get firing rates\n",
    "        rates = {n: d['rate'] for n, d in G.nodes(data=True)}\n",
    "\n",
    "        # Map firing rates to node sizes (you can adjust the scaling factor as needed)\n",
    "        node_sizes = [50 * rate for rate in rates.values()]\n",
    "\n",
    "        # Normalize firing rates to the range 0 to 1 for color mapping\n",
    "        max_rate = max(rates.values())\n",
    "        normalized_rates = [rate / max_rate for rate in rates.values()]\n",
    "\n",
    "        # Map normalized firing rates to colors using a color map\n",
    "        node_colors = [cm.hot(rate) for rate in normalized_rates]\n",
    "\n",
    "        # Create a list of edges to remove\n",
    "        edges_to_remove = [(u, v) for u, v in G.edges() if rates[u] <= 0 and rates[v] <= 0]\n",
    "\n",
    "        # Remove the edges\n",
    "        G.remove_edges_from(edges_to_remove)\n",
    "\n",
    "        # Get edge weights\n",
    "        edge_weights = [d['weight'] for u, v, d in G.edges(data=True)]\n",
    "\n",
    "        # Map edge weights to colors using a color map\n",
    "        edge_colors = [cm.cool(weight) for weight in edge_weights]\n",
    "\n",
    "        # Draw the graph\n",
    "        plt.gca().set_facecolor('black')\n",
    "        pos = nx.kamada_kawai_layout(G)\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.7)\n",
    "        nx.draw_networkx_edges(G, pos, edge_color=edge_colors, alpha=1)\n",
    "\n",
    "        # Save the figure to a file in the output folder\n",
    "        image_file_path = os.path.join(output_folder, f\"graph_{i}.png\")\n",
    "        plt.savefig(image_file_path, facecolor='black')\n",
    "        image_file_paths.append(image_file_path)\n",
    "\n",
    "        # Clear the figure so that the next graph is not drawn on top of this one\n",
    "        plt.clf()\n",
    "\n",
    "    # Sort the image file paths numerically based on the index value\n",
    "    image_file_paths.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "\n",
    "    # Create a GIF\n",
    "    gif_file_path = os.path.join(output_folder, \"graph_sequence.gif\")\n",
    "    with imageio.get_writer(gif_file_path, mode='I', loop=0) as writer:\n",
    "        for image_file_path in image_file_paths:\n",
    "            image = imageio.imread(image_file_path)\n",
    "            writer.append_data(image)\n",
    "\n",
    "    # Display the GIF\n",
    "    display(Image(filename=gif_file_path))\n",
    "\n",
    "    \n",
    "generate_graphs(num_graphs=10, output_folder=\"output_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# List to store the file paths of the images\n",
    "image_file_paths = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Load the graph\n",
    "    with open(os.path.join(\"graphs_721123822\", f\"graph_{i}.pkl\"), 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Get firing rates\n",
    "    rates = {n: d['rate'] for n, d in G.nodes(data=True)}\n",
    "\n",
    "    # Map firing rates to node sizes (you can adjust the scaling factor as needed)\n",
    "    node_sizes = [50 * rate for rate in rates.values()]\n",
    "\n",
    "    # Normalize firing rates to the range 0 to 1 for color mapping\n",
    "    max_rate = max(rates.values())\n",
    "    normalized_rates = [rate / max_rate for rate in rates.values()]\n",
    "\n",
    "    # Map normalized firing rates to colors using a color map\n",
    "    node_colors = [cm.hot(rate) for rate in normalized_rates]\n",
    "\n",
    "    # Create a list of edges to remove\n",
    "    edges_to_remove = [(u, v) for u, v in G.edges() if rates[u] <= 0 and rates[v] <= 0]\n",
    "\n",
    "    # Remove the edges\n",
    "    G.remove_edges_from(edges_to_remove)\n",
    "\n",
    "    # Get edge weights\n",
    "    edge_weights = [d['weight'] for u, v, d in G.edges(data=True)]\n",
    "\n",
    "    # Map edge weights to colors using a color map\n",
    "    edge_colors = [cm.cool(weight) for weight in edge_weights]\n",
    "\n",
    "    # Draw the graph\n",
    "    plt.gca().set_facecolor('black')\n",
    "    pos = nx.kamada_kawai_layout(G)\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.7)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, alpha=1)\n",
    "\n",
    "    # Save the figure to a file\n",
    "    image_file_path = f\"graph_{i}.png\"\n",
    "    plt.savefig(image_file_path, facecolor='black')\n",
    "    image_file_paths.append(image_file_path)\n",
    "\n",
    "    # Clear the figure so that the next graph is not drawn on top of this one\n",
    "    plt.clf()\n",
    "\n",
    "# Sort the image file paths numerically based on the index value\n",
    "image_file_paths.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "\n",
    "# Create a GIF\n",
    "with imageio.get_writer('graph_sequence.gif', mode='I', loop=0) as writer:\n",
    "    for image_file_path in image_file_paths:\n",
    "        image = imageio.imread(image_file_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "# Display the GIF\n",
    "display(Image(filename='graph_sequence.gif'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# List to store the file paths of the images\n",
    "image_file_paths = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Load the graph\n",
    "    with open(os.path.join(\"graphs_721123822\", f\"graph_{i}.pkl\"), 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Get firing rates\n",
    "    rates = {n: d['rate'] for n, d in G.nodes(data=True)}\n",
    "\n",
    "    # Map firing rates to node sizes (you can adjust the scaling factor as needed)\n",
    "    node_sizes = [50 * rate for rate in rates.values()]\n",
    "\n",
    "    # Normalize firing rates to the range 0 to 1 for color mapping\n",
    "    max_rate = max(rates.values())\n",
    "    normalized_rates = [rate / max_rate for rate in rates.values()]\n",
    "\n",
    "    # Map normalized firing rates to colors using a color map\n",
    "    node_colors = [cm.hot(rate) for rate in normalized_rates]\n",
    "\n",
    "    # Create a list of edges to remove\n",
    "    edges_to_remove = [(u, v) for u, v in G.edges() if rates[u] <= 0 and rates[v] <= 0]\n",
    "\n",
    "    # Remove the edges\n",
    "    G.remove_edges_from(edges_to_remove)\n",
    "\n",
    "    # Get edge weights\n",
    "    edge_weights = [d['weight'] for u, v, d in G.edges(data=True)]\n",
    "\n",
    "    # Map edge weights to colors using a color map\n",
    "    edge_colors = [cm.cool(weight) for weight in edge_weights]\n",
    "\n",
    "    # Draw the graph\n",
    "    plt.gca().set_facecolor('black')\n",
    "    pos = nx.random_layout(G, seed = 69)\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.7)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, alpha=1)\n",
    "\n",
    "    # Save the figure to a file\n",
    "    image_file_path = f\"graph_{i}.png\"\n",
    "    plt.savefig(image_file_path, facecolor='black')\n",
    "    image_file_paths.append(image_file_path)\n",
    "\n",
    "    # Clear the figure so that the next graph is not drawn on top of this one\n",
    "    plt.clf()\n",
    "\n",
    "# Sort the image file paths numerically based on the index value\n",
    "image_file_paths.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "\n",
    "# Create a GIF\n",
    "with imageio.get_writer('graph_sequence.gif', mode='I', loop=0) as writer:\n",
    "    for image_file_path in image_file_paths:\n",
    "        image = imageio.imread(image_file_path)\n",
    "        writer.append_data(image)\n",
    "\n",
    "# Display the GIF\n",
    "display(Image(filename='graph_sequence.gif'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phd52D6yLKgJ",
    "tags": []
   },
   "source": [
    "<a name=\"references\"></a>\n",
    "# References \n",
    "[Go to Outline](#outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neuropixel References\n",
    "\n",
    "- Data white paper: Allen Brain Observatory – Neuropixels Visual Coding, https://brainmapportal-live-4cc80a57cd6e400d854-f7fdcae.divio-media.net/filer_public/80/75/8075a100-ca64-429a-b39a-569121b612b2/neuropixels_visual_coding_-_white_paper_v10.pdf\n",
    "- neuropixel data overview: Large-scale neural recordings with single neuron resolution using Neuropixels probes in human cortex,https://www.nature.com/articles/s41593-021-00997-0\n",
    "- more neuropixel data overview: Fully integrated silicon probes for high-density recording of neural activity, https://www.nature.com/articles/nature24636\n",
    "- synaptic transmission: https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/synaptic-transmission\n",
    "- Uses same type of data(mouse visual cortex neurons w/ HD-MEA): https://www.nature.com/articles/s41586-019-1346-5\n",
    "     - Shows similar images may invoke similar responses to save on personal computation.\n",
    "- Study on same Dataset, uses 'Gain' model. https://elifesciences.org/articles/77907#s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM References\n",
    "\n",
    "- Paper model overview: http://www.bioinf.jku.at/publications/older/2604.pdf \n",
    "- Another Paper: https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf \n",
    "- Explanation - This guy worked at the stats dept at UNC I believe. https://www.youtube.com/watch?v=YCzL96nL7j0&ab_channel=StatQuestwithJoshStarmer \n",
    "- step by step: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Spatio-Temporal GNN References: \n",
    "\n",
    "- How attentive are graph attention networks? https://arxiv.org/pdf/2105.14491.pdf\n",
    "- Adaptive weights for adjacency matrix in STGNN: https://www.ijcai.org/proceedings/2019/0264.pdf\n",
    "- Discertation\"Applications of Spatio-Temporal Graph Neural Network Models for Brain Connectivity Analysis\": https://epub.uni-regensburg.de/53477/1/Dissertation_SimonWein.pdf\n",
    "- Video reference: https://www.youtube.com/watch?v=RRMU8kJH60Q&t=250s&ab_channel=JacobHeglund\n",
    "    - Code from video: https://drive.google.com/file/d/1WpBOZlDiDTTeeXN1gr31TMCWeh4qWW7W/view\n",
    "- Attention Based Spatial-Temporal Graph Convolutional Networks(ASTGCN) - ***Paper referenced in video above: https://guoshnbjtu.github.io/pdfs/AAAI2019-GuoS.2690.pdf\n",
    "- STGNN for diagnosis of Depression (fMRI):https://onlinelibrary.wiley.com/doi/10.1002/hbm.25529\n",
    "- TEMPORAL GRAPH NETWORKS FOR DEEP LEARNING ON DYNAMIC GRAPHS: https://arxiv.org/pdf/2006.10637.pdf\n",
    "- Another Video Reference: https://www.youtube.com/watch?v=WEWq93tioC4&ab_channel=DeepFindr\n",
    "- Older paper on Temporal Graph Networks: https://arxiv.org/pdf/2006.10637.pdf\n",
    "- Pytorch and STGNNs: https://arxiv.org/pdf/2104.07788.pdf\n",
    "- Survey of STGNNs: https://arxiv.org/abs/2301.10569\n",
    "\n",
    "- STGNN predicting resting states in fMRI: https://www.sciencedirect.com/science/article/pii/S1361841522001189\n",
    "- Spatio-Temporal Graph Convolution for Resting-State fMRI Analysis https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7700758/\n",
    "- STGNN for flood forecasting: https://www.nature.com/articles/s41598-023-32548-x\n",
    "- Other types of predictive Graph Networks - https://jonathan-hui.medium.com/graph-neural-networks-gnn-gae-stgnn-1ac0b5c99550\n",
    "- \"Graph neural networks: A review of methods and applications\":https://www.sciencedirect.com/science/article/pii/S2666651021000012\n",
    "\n",
    "##### Learnable Adjacency Matrix\n",
    "\n",
    "- Overview of GAT: https://arxiv.org/abs/1710.10903\n",
    "- Adaptive GCNN: https://arxiv.org/abs/1801.03226"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functional Connectomic References\n",
    "\n",
    "- Functional brain networks reflect spatial and temporal autocorrelation(fMRI study): https://www.nature.com/articles/s41593-023-01299-3\n",
    "- Using GNN to show changes in functional connectivity : https://www.frontiersin.org/articles/10.3389/fninf.2022.1032538/full\n",
    "            - The above paper shows that showing the funcitonal connectivity in a GNN using graphsage is useful for predicting drug changes in functional connectivit.\n",
    "            - They used umap in python to show groupings related to change in states.\n",
    "- A graph neural network framework for causal inference in brain networks(fMRI study): https://www.nature.com/articles/s41598-021-87411-8\n",
    "- Distinct brain-wide presynaptic networks underlie the functional identity of individual cortical neurons : https://www.biorxiv.org/content/10.1101/2023.05.25.542329v1\n",
    "- Overview of other models used to find Functional Connectomics in the mouse visual cortex: https://www.biorxiv.org/content/10.1101/662189v2.full\n",
    "- Older paper on overview on identifying Functional Connectivity in single neurons:https://direct.mit.edu/neco/article-abstract/21/2/450/8696/Identifying-Functional-Connectivity-in-Large-Scale?redirectedFrom=fulltext\n",
    "- Learning-Induced Enduring Changes in Functional Connectivity among Prefrontal Cortical Neurons: https://www.jneurosci.org/content/27/4/909"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Possible Future models\n",
    "- Graph LSTMs\n",
    "- Generating model for GNN: Discovering Symbolic Models from Deep Learning with Inductive Biases - https://www.youtube.com/watch?v=LMb5tvW-UoQ&ab_channel=YannicKilcher \n",
    "        -paper: https://arxiv.org/abs/2006.11287 \n",
    "        -Code: https://github.com/MilesCranmer/symbolic_deep_learning\n",
    "- Method to scale STGNNs to compute more timesteps: https://arxiv.org/abs/2209.06520\n",
    "- Physics Informed Neural Net(PINN): https://arxiv.org/abs/2201.05624 \n",
    "        - Similar to Discovering Symbolic Models from Deep Learning with Inductive Biases: https://www.youtube.com/watch?v=LMb5tvW-UoQ&t=1217s&ab_channel=YannicKilcher \n",
    "        - Converting NNs to symbolic models \"https://www.youtube.com/watch?v=wmQIcTOzH0k&ab_channel=MilesCranmer\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ExcZz5n4lcWT",
    "IQUalmkolwzs",
    "9bbQB_WBgPQG",
    "BQ0IJMbEgPrb",
    "x86qhjr_gp7k",
    "4pXhgHnng0bL",
    "o-AKtrRqlo91",
    "_PaKK-0Olo95",
    "XZ3EgcT8yycD",
    "fc37wCOHr59L",
    "g-89IKnQlo97",
    "Mav5i5bbt938",
    "EBInmft27Rzv",
    "Vv2LEyG-7Stt",
    "IK16DzNQnOY7",
    "X1VYP2b-lo-A",
    "UikqgS7gGqOH"
   ],
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
